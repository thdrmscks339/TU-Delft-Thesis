{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a258b663-9008-41e7-86a5-7161af6eee72",
   "metadata": {},
   "source": [
    "# Convert LST to Air Temperature with MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fba87ab-9d47-471b-8419-253253af8651",
   "metadata": {},
   "source": [
    "LST and 4 Input Features Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c6947aa-d71b-4a37-b992-73adeb3aea9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose an option:\n",
      "1. Clean to new folder (safer)\n",
      "2. Clean in-place (overwrites originals after backup)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter choice (1 or 2):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating backup of original files in: /Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/Input Layer/Seoul Landsat_backup\n",
      "Backup complete!\n",
      "Found 95 files to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:   1%|▎                       | 1/95 [00:00<00:15,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2017_06_L8.tif: 3,801 outliers (0.3%) removed\n",
      "    LST range: [11.8, 64.3]°C\n",
      "    IQR bounds: [25.7, 53.6]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:   2%|▌                       | 2/95 [00:00<00:13,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2018_07_L8.tif: 5,411 outliers (0.6%) removed\n",
      "    LST range: [-60.6, 65.2]°C\n",
      "    IQR bounds: [23.0, 54.4]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:   3%|▊                       | 3/95 [00:00<00:11,  8.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2021_01_L8.tif: 68 outliers (0.0%) removed\n",
      "    LST range: [-17.7, 0.1]°C\n",
      "    IQR bounds: [-17.4, -0.7]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:   4%|█                       | 4/95 [00:00<00:11,  7.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2013_09_L8.tif: 710 outliers (0.0%) removed\n",
      "    LST range: [18.0, 54.7]°C\n",
      "    IQR bounds: [18.1, 43.5]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:   5%|█▎                      | 5/95 [00:00<00:10,  8.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2014_01_L8.tif: 37,165 outliers (9.1%) removed\n",
      "    LST range: [-10.1, 9.2]°C\n",
      "    IQR bounds: [-4.5, 5.0]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:   6%|█▌                      | 6/95 [00:00<00:11,  7.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2013_05_L8.tif: 949 outliers (0.1%) removed\n",
      "    LST range: [11.1, 51.0]°C\n",
      "    IQR bounds: [16.0, 38.4]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:   7%|█▊                      | 7/95 [00:00<00:11,  7.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2024_08_L8.tif: 48 outliers (0.0%) removed\n",
      "    LST range: [27.4, 59.0]°C\n",
      "    IQR bounds: [22.9, 54.6]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:   8%|██                      | 8/95 [00:01<00:11,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2023_12_L8.tif: 49,440 outliers (3.2%) removed\n",
      "    LST range: [-14.4, 12.6]°C\n",
      "    IQR bounds: [-3.1, 6.0]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:   9%|██▎                     | 9/95 [00:01<00:12,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2015_07_L8.tif: 447 outliers (0.0%) removed\n",
      "    LST range: [11.3, 60.6]°C\n",
      "    IQR bounds: [18.9, 51.5]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  11%|██▍                    | 10/95 [00:01<00:12,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2024_04_L8.tif: 65,926 outliers (4.2%) removed\n",
      "    LST range: [10.8, 44.0]°C\n",
      "    IQR bounds: [19.9, 32.3]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  12%|██▋                    | 11/95 [00:01<00:11,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2020_07_L8.tif: 1,391 outliers (0.1%) removed\n",
      "    LST range: [12.1, 55.2]°C\n",
      "    IQR bounds: [21.1, 48.8]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  13%|██▉                    | 12/95 [00:01<00:11,  7.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2016_12_L8.tif: 32,345 outliers (3.1%) removed\n",
      "    LST range: [-10.9, 8.1]°C\n",
      "    IQR bounds: [-6.0, 3.9]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  14%|███▏                   | 13/95 [00:01<00:11,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2019_01_L8.tif: 56,540 outliers (3.7%) removed\n",
      "    LST range: [-8.6, 14.1]°C\n",
      "    IQR bounds: [-0.9, 7.8]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  15%|███▍                   | 14/95 [00:01<00:11,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2024_06_L8.tif: 298 outliers (0.0%) removed\n",
      "    LST range: [15.6, 58.0]°C\n",
      "    IQR bounds: [20.5, 52.5]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  16%|███▋                   | 15/95 [00:02<00:11,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2015_09_L8.tif: 5,256 outliers (0.3%) removed\n",
      "    LST range: [18.6, 52.5]°C\n",
      "    IQR bounds: [22.0, 41.0]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  17%|███▊                   | 16/95 [00:02<00:11,  6.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2015_05_L8.tif: 32,091 outliers (2.0%) removed\n",
      "    LST range: [18.3, 53.6]°C\n",
      "    IQR bounds: [21.1, 42.1]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  18%|████                   | 17/95 [00:02<00:11,  6.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2023_10_L8.tif: 14,424 outliers (0.9%) removed\n",
      "    LST range: [6.7, 34.1]°C\n",
      "    IQR bounds: [15.1, 26.4]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  19%|████▎                  | 18/95 [00:02<00:11,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2020_09_L8.tif: 1,212 outliers (0.1%) removed\n",
      "    LST range: [18.0, 47.0]°C\n",
      "    IQR bounds: [18.1, 41.2]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  20%|████▌                  | 19/95 [00:02<00:10,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2016_10_L8.tif: 6,206 outliers (0.4%) removed\n",
      "    LST range: [9.8, 40.6]°C\n",
      "    IQR bounds: [13.0, 28.6]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  21%|████▊                  | 20/95 [00:02<00:10,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2020_05_L8.tif: 207 outliers (0.0%) removed\n",
      "    LST range: [13.7, 57.0]°C\n",
      "    IQR bounds: [19.6, 50.4]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  22%|█████                  | 21/95 [00:02<00:10,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2019_11_L8.tif: 36,125 outliers (2.3%) removed\n",
      "    LST range: [-8.7, 19.3]°C\n",
      "    IQR bounds: [1.3, 11.4]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  23%|█████▎                 | 22/95 [00:03<00:10,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2021_03_L8.tif: 89,841 outliers (5.7%) removed\n",
      "    LST range: [9.8, 38.6]°C\n",
      "    IQR bounds: [15.6, 27.4]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  24%|█████▌                 | 23/95 [00:03<00:10,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2017_08_L8.tif: 4,413 outliers (0.3%) removed\n",
      "    LST range: [-2.1, 21.9]°C\n",
      "    IQR bounds: [-0.4, 19.4]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  25%|█████▊                 | 24/95 [00:03<00:09,  7.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2018_09_L8.tif: 4,251 outliers (0.5%) removed\n",
      "    LST range: [1.8, 45.0]°C\n",
      "    IQR bounds: [16.7, 32.7]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  26%|██████                 | 25/95 [00:03<00:09,  7.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2018_05_L8.tif: 536 outliers (0.0%) removed\n",
      "    LST range: [15.0, 54.3]°C\n",
      "    IQR bounds: [12.8, 41.7]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  27%|██████▎                | 26/95 [00:03<00:09,  7.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2021_11_L8.tif: 27,139 outliers (2.0%) removed\n",
      "    LST range: [-0.2, 20.8]°C\n",
      "    IQR bounds: [6.3, 14.2]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  28%|██████▌                | 27/95 [00:03<00:09,  7.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2017_04_L8.tif: 62,426 outliers (4.1%) removed\n",
      "    LST range: [6.3, 40.2]°C\n",
      "    IQR bounds: [15.9, 29.4]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  29%|██████▊                | 28/95 [00:03<00:09,  7.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2014_03_L8.tif: 61,243 outliers (3.9%) removed\n",
      "    LST range: [7.9, 39.3]°C\n",
      "    IQR bounds: [17.8, 28.5]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  31%|███████                | 29/95 [00:03<00:08,  7.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2022_04_L8.tif: 82,080 outliers (5.2%) removed\n",
      "    LST range: [6.1, 37.9]°C\n",
      "    IQR bounds: [14.0, 27.5]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  32%|███████▎               | 30/95 [00:04<00:08,  7.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2014_11_L8.tif: 25,454 outliers (1.7%) removed\n",
      "    LST range: [2.3, 28.8]°C\n",
      "    IQR bounds: [13.8, 21.6]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  33%|███████▌               | 31/95 [00:04<00:08,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2020_01_L8.tif: 50,325 outliers (3.2%) removed\n",
      "    LST range: [-8.2, 19.2]°C\n",
      "    IQR bounds: [4.0, 12.0]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  34%|███████▋               | 32/95 [00:04<00:08,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2015_01_L8.tif: 36,391 outliers (2.7%) removed\n",
      "    LST range: [-13.8, 11.4]°C\n",
      "    IQR bounds: [-2.1, 5.1]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  35%|███████▉               | 33/95 [00:04<00:08,  7.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2024_10_L8.tif: 5,134 outliers (0.4%) removed\n",
      "    LST range: [1.3, 39.8]°C\n",
      "    IQR bounds: [14.3, 31.3]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  36%|████████▏              | 34/95 [00:04<00:07,  7.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2023_06_L8.tif: 233 outliers (0.0%) removed\n",
      "    LST range: [18.8, 58.0]°C\n",
      "    IQR bounds: [20.7, 48.5]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  37%|████████▍              | 35/95 [00:04<00:07,  7.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2022_12_L8.tif: 23,140 outliers (2.3%) removed\n",
      "    LST range: [-9.0, 11.5]°C\n",
      "    IQR bounds: [-4.2, 6.3]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  38%|████████▋              | 36/95 [00:04<00:07,  8.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2014_07_L8.tif: 292 outliers (0.0%) removed\n",
      "    LST range: [18.6, 58.0]°C\n",
      "    IQR bounds: [21.1, 51.9]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  39%|████████▉              | 37/95 [00:04<00:07,  7.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2018_01_L8.tif: 65,581 outliers (4.5%) removed\n",
      "    LST range: [-13.2, 16.1]°C\n",
      "    IQR bounds: [-1.7, 6.0]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  40%|█████████▏             | 38/95 [00:05<00:07,  7.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2017_12_L8.tif: 57,943 outliers (4.2%) removed\n",
      "    LST range: [-17.9, 9.7]°C\n",
      "    IQR bounds: [-5.7, 3.5]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  41%|█████████▍             | 39/95 [00:05<00:07,  7.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2014_05_L8.tif: 429 outliers (0.0%) removed\n",
      "    LST range: [22.0, 57.3]°C\n",
      "    IQR bounds: [21.4, 49.6]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  42%|█████████▋             | 40/95 [00:05<00:07,  7.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2022_10_L8.tif: 5,599 outliers (0.4%) removed\n",
      "    LST range: [6.6, 34.4]°C\n",
      "    IQR bounds: [10.6, 26.0]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  43%|█████████▉             | 41/95 [00:05<00:07,  7.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2014_09_L8.tif: 1,514 outliers (0.1%) removed\n",
      "    LST range: [16.3, 53.1]°C\n",
      "    IQR bounds: [19.0, 40.9]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  44%|██████████▏            | 42/95 [00:05<00:07,  7.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2017_10_L8.tif: 7,303 outliers (0.5%) removed\n",
      "    LST range: [9.2, 31.2]°C\n",
      "    IQR bounds: [13.8, 24.0]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  45%|██████████▍            | 43/95 [00:05<00:06,  7.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2018_11_L8.tif: 18,886 outliers (1.2%) removed\n",
      "    LST range: [0.4, 32.0]°C\n",
      "    IQR bounds: [9.9, 20.7]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  46%|██████████▋            | 44/95 [00:05<00:06,  7.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2017_02_L8.tif: 52,192 outliers (3.4%) removed\n",
      "    LST range: [-9.0, 22.6]°C\n",
      "    IQR bounds: [2.8, 11.4]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  47%|██████████▉            | 45/95 [00:06<00:06,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2018_03_L8.tif: 54,871 outliers (3.5%) removed\n",
      "    LST range: [-0.5, 32.1]°C\n",
      "    IQR bounds: [6.1, 18.2]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  48%|███████████▏           | 46/95 [00:06<00:06,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2016_04_L8.tif: 85,871 outliers (5.4%) removed\n",
      "    LST range: [13.4, 46.9]°C\n",
      "    IQR bounds: [22.0, 33.2]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  49%|███████████▍           | 47/95 [00:06<00:06,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2020_03_L8.tif: 48,018 outliers (3.0%) removed\n",
      "    LST range: [-2.0, 26.0]°C\n",
      "    IQR bounds: [5.4, 17.8]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  51%|███████████▌           | 48/95 [00:06<00:06,  7.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2016_08_L8.tif: 518 outliers (0.0%) removed\n",
      "    LST range: [20.4, 66.3]°C\n",
      "    IQR bounds: [23.8, 55.8]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  52%|███████████▊           | 49/95 [00:06<00:05,  7.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2023_04_L8.tif: 578 outliers (0.1%) removed\n",
      "    LST range: [9.2, 38.6]°C\n",
      "    IQR bounds: [12.9, 33.5]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  53%|████████████           | 50/95 [00:06<00:05,  7.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2015_03_L8.tif: 82,500 outliers (5.2%) removed\n",
      "    LST range: [-0.2, 34.7]°C\n",
      "    IQR bounds: [8.7, 21.3]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  54%|████████████▎          | 51/95 [00:06<00:05,  7.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2024_12_L8.tif: 42,011 outliers (2.8%) removed\n",
      "    LST range: [-13.9, 12.9]°C\n",
      "    IQR bounds: [-2.8, 5.6]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  55%|████████████▌          | 52/95 [00:07<00:05,  7.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2019_12_L8.tif: 58,972 outliers (3.9%) removed\n",
      "    LST range: [-17.1, 11.5]°C\n",
      "    IQR bounds: [-4.8, 4.8]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  56%|████████████▊          | 53/95 [00:07<00:05,  7.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2016_01_L8.tif: 62,125 outliers (4.1%) removed\n",
      "    LST range: [-15.1, 13.7]°C\n",
      "    IQR bounds: [-3.8, 4.9]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  57%|█████████████          | 54/95 [00:07<00:05,  7.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2015_06_L8.tif: 348 outliers (0.0%) removed\n",
      "    LST range: [17.8, 53.4]°C\n",
      "    IQR bounds: [19.4, 47.4]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  58%|█████████████▎         | 55/95 [00:07<00:05,  7.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2024_09_L8.tif: 4,590 outliers (0.3%) removed\n",
      "    LST range: [12.5, 45.8]°C\n",
      "    IQR bounds: [16.6, 42.1]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  59%|█████████████▌         | 56/95 [00:07<00:05,  7.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2024_05_L8.tif: 185 outliers (0.0%) removed\n",
      "    LST range: [8.3, 45.2]°C\n",
      "    IQR bounds: [14.4, 38.7]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  60%|█████████████▊         | 57/95 [00:07<00:04,  7.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2014_12_L8.tif: 60,745 outliers (4.8%) removed\n",
      "    LST range: [-11.8, 13.5]°C\n",
      "    IQR bounds: [-0.5, 7.3]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  61%|██████████████         | 58/95 [00:07<00:04,  7.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2021_12_L8.tif: 34,553 outliers (2.3%) removed\n",
      "    LST range: [-18.8, 7.9]°C\n",
      "    IQR bounds: [-7.6, 2.0]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  62%|██████████████▎        | 59/95 [00:07<00:04,  7.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2017_07_L8.tif: 1,936 outliers (0.2%) removed\n",
      "    LST range: [19.7, 67.9]°C\n",
      "    IQR bounds: [24.1, 50.7]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  63%|██████████████▌        | 60/95 [00:08<00:04,  7.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2014_02_L8.tif: 37,967 outliers (3.0%) removed\n",
      "    LST range: [-4.8, 24.1]°C\n",
      "    IQR bounds: [5.6, 12.9]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  64%|██████████████▊        | 61/95 [00:08<00:04,  7.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2022_09_L8.tif: 22,077 outliers (1.5%) removed\n",
      "    LST range: [-3.7, 51.4]°C\n",
      "    IQR bounds: [18.8, 44.7]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  65%|███████████████        | 62/95 [00:08<00:04,  7.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2013_06_L8.tif: 565 outliers (0.0%) removed\n",
      "    LST range: [21.2, 65.5]°C\n",
      "    IQR bounds: [20.0, 52.5]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  66%|███████████████▎       | 63/95 [00:08<00:04,  7.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2014_10_L8.tif: 1,879 outliers (0.1%) removed\n",
      "    LST range: [12.7, 43.8]°C\n",
      "    IQR bounds: [15.4, 33.4]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  67%|███████████████▍       | 64/95 [00:08<00:03,  7.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2022_05_L8.tif: 380 outliers (0.0%) removed\n",
      "    LST range: [14.6, 47.6]°C\n",
      "    IQR bounds: [17.6, 40.1]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  68%|███████████████▋       | 65/95 [00:08<00:03,  7.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2021_02_L8.tif: 67,868 outliers (4.3%) removed\n",
      "    LST range: [2.8, 30.5]°C\n",
      "    IQR bounds: [8.9, 19.9]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  69%|███████████████▉       | 66/95 [00:08<00:03,  7.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2017_05_L8.tif: 353 outliers (0.0%) removed\n",
      "    LST range: [11.8, 43.5]°C\n",
      "    IQR bounds: [12.7, 39.1]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  71%|████████████████▏      | 67/95 [00:08<00:03,  7.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2021_10_L8.tif: 6,516 outliers (0.4%) removed\n",
      "    LST range: [7.9, 34.0]°C\n",
      "    IQR bounds: [11.9, 25.5]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  72%|████████████████▍      | 68/95 [00:09<00:03,  7.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2019_02_L8.tif: 42,405 outliers (2.9%) removed\n",
      "    LST range: [-4.2, 24.3]°C\n",
      "    IQR bounds: [3.0, 12.8]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  73%|████████████████▋      | 69/95 [00:09<00:03,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2016_03_L8.tif: 91,358 outliers (5.8%) removed\n",
      "    LST range: [4.7, 37.2]°C\n",
      "    IQR bounds: [12.4, 22.8]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  74%|████████████████▉      | 70/95 [00:09<00:03,  7.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2019_10_L8.tif: 8,100 outliers (0.5%) removed\n",
      "    LST range: [7.9, 40.8]°C\n",
      "    IQR bounds: [16.8, 31.6]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  75%|█████████████████▏     | 71/95 [00:09<00:03,  7.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2020_04_L8.tif: 37,414 outliers (2.4%) removed\n",
      "    LST range: [10.4, 45.0]°C\n",
      "    IQR bounds: [16.8, 36.4]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  76%|█████████████████▍     | 72/95 [00:09<00:02,  7.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2016_11_L8.tif: 28,233 outliers (2.3%) removed\n",
      "    LST range: [-4.8, 26.1]°C\n",
      "    IQR bounds: [9.1, 17.9]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  77%|█████████████████▋     | 73/95 [00:09<00:02,  7.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2017_01_L8.tif: 69,290 outliers (4.6%) removed\n",
      "    LST range: [-18.5, 8.2]°C\n",
      "    IQR bounds: [-7.4, 2.2]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  78%|█████████████████▉     | 74/95 [00:09<00:02,  7.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2018_12_L8.tif: 41,125 outliers (2.8%) removed\n",
      "    LST range: [-8.3, 17.9]°C\n",
      "    IQR bounds: [3.3, 11.1]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  79%|██████████████████▏    | 75/95 [00:09<00:02,  8.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2021_06_L8.tif: 9,570 outliers (1.3%) removed\n",
      "    LST range: [5.0, 53.5]°C\n",
      "    IQR bounds: [12.0, 38.4]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  80%|██████████████████▍    | 76/95 [00:10<00:02,  7.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2022_01_L8.tif: 49,499 outliers (3.3%) removed\n",
      "    LST range: [-15.4, 15.6]°C\n",
      "    IQR bounds: [-1.7, 7.8]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  81%|██████████████████▋    | 77/95 [00:10<00:02,  7.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2015_12_L8.tif: 59,967 outliers (4.0%) removed\n",
      "    LST range: [-16.1, 10.8]°C\n",
      "    IQR bounds: [-4.9, 4.4]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  82%|██████████████████▉    | 78/95 [00:10<00:02,  7.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2020_12_L8.tif: 45,860 outliers (3.3%) removed\n",
      "    LST range: [-10.0, 16.9]°C\n",
      "    IQR bounds: [2.6, 9.8]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  83%|███████████████████▏   | 79/95 [00:10<00:02,  7.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2019_06_L8.tif: 454 outliers (0.0%) removed\n",
      "    LST range: [16.6, 58.9]°C\n",
      "    IQR bounds: [17.7, 50.0]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  84%|███████████████████▎   | 80/95 [00:10<00:01,  7.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2016_07_L8.tif: 384 outliers (0.0%) removed\n",
      "    LST range: [25.4, 65.9]°C\n",
      "    IQR bounds: [23.4, 56.0]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  85%|███████████████████▌   | 81/95 [00:10<00:01,  7.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2024_01_L8.tif: 7,010 outliers (0.6%) removed\n",
      "    LST range: [-10.9, 15.0]°C\n",
      "    IQR bounds: [-1.3, 10.1]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  86%|███████████████████▊   | 82/95 [00:10<00:01,  7.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2015_10_L8.tif: 10,559 outliers (1.0%) removed\n",
      "    LST range: [6.6, 42.1]°C\n",
      "    IQR bounds: [17.2, 32.4]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  87%|████████████████████   | 83/95 [00:11<00:01,  7.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2023_05_L8.tif: 46 outliers (0.0%) removed\n",
      "    LST range: [15.0, 50.6]°C\n",
      "    IQR bounds: [15.7, 47.9]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  88%|████████████████████▎  | 84/95 [00:11<00:01,  7.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2015_02_L8.tif: 12,544 outliers (1.0%) removed\n",
      "    LST range: [-8.5, 24.8]°C\n",
      "    IQR bounds: [0.8, 14.7]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  89%|████████████████████▌  | 85/95 [00:11<00:01,  8.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2023_09_L8.tif: 2,506 outliers (0.3%) removed\n",
      "    LST range: [11.1, 44.6]°C\n",
      "    IQR bounds: [18.1, 34.8]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  91%|████████████████████▊  | 86/95 [00:11<00:01,  8.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2016_05_L8.tif: 889 outliers (0.1%) removed\n",
      "    LST range: [21.6, 63.0]°C\n",
      "    IQR bounds: [21.7, 50.5]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  92%|█████████████████████  | 87/95 [00:11<00:01,  7.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2020_10_L8.tif: 2,268 outliers (0.1%) removed\n",
      "    LST range: [8.4, 37.5]°C\n",
      "    IQR bounds: [12.9, 30.7]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  93%|█████████████████████▎ | 88/95 [00:11<00:00,  7.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2020_02_L8.tif: 53,013 outliers (3.5%) removed\n",
      "    LST range: [-12.3, 16.8]°C\n",
      "    IQR bounds: [0.5, 10.1]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  94%|█████████████████████▌ | 89/95 [00:11<00:00,  7.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2016_09_L8.tif: 1,610 outliers (0.1%) removed\n",
      "    LST range: [20.6, 52.5]°C\n",
      "    IQR bounds: [21.0, 39.6]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  95%|█████████████████████▊ | 90/95 [00:11<00:00,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2021_04_L8.tif: 1,783 outliers (0.1%) removed\n",
      "    LST range: [8.1, 39.0]°C\n",
      "    IQR bounds: [10.5, 32.1]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  96%|██████████████████████ | 91/95 [00:12<00:00,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2017_11_L8.tif: 48,756 outliers (3.1%) removed\n",
      "    LST range: [-12.1, 16.2]°C\n",
      "    IQR bounds: [-1.0, 8.3]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  97%|██████████████████████▎| 92/95 [00:12<00:00,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2018_02_L8.tif: 42,026 outliers (3.0%) removed\n",
      "    LST range: [-11.9, 17.8]°C\n",
      "    IQR bounds: [-2.8, 6.9]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  98%|██████████████████████▌| 93/95 [00:12<00:00,  7.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2021_08_L8.tif: 804 outliers (0.1%) removed\n",
      "    LST range: [13.9, 56.1]°C\n",
      "    IQR bounds: [21.5, 51.8]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files:  99%|██████████████████████▊| 94/95 [00:12<00:00,  7.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2017_03_L8.tif: 80,266 outliers (5.1%) removed\n",
      "    LST range: [7.2, 39.5]°C\n",
      "    IQR bounds: [14.6, 25.8]°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning LST files: 100%|███████████████████████| 95/95 [00:12<00:00,  7.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seoul_2013_12_L8.tif: 53,363 outliers (5.8%) removed\n",
      "    LST range: [-13.5, 12.1]°C\n",
      "    IQR bounds: [-1.7, 5.0]°C\n",
      "\n",
      "=== CLEANING SUMMARY ===\n",
      "Total files processed: 95\n",
      "Total pixels processed: 134,480,973\n",
      "Total outliers removed: 2,508,933\n",
      "Overall outlier percentage: 1.87%\n",
      "Cleaned files saved to: /Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/Input Layer/Seoul Landsat Cleaned\n",
      "\n",
      "IMPORTANT: Update your SATELLITE_FOLDER path to use the cleaned folder!\n",
      "\n",
      "✓ Done! Update your scripts to use: '/Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/Input Layer/Seoul Landsat Cleaned'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "def clean_lst_outliers_iqr(input_folder, output_folder, multiplier=1.5, create_backup=True):\n",
    "    \"\"\"\n",
    "    Remove LST outliers from all GeoTIFF files using IQR method.\n",
    "    Saves cleaned files with SAME names as originals.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output folder\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Optional: Create backup of originals\n",
    "    if create_backup:\n",
    "        backup_folder = input_folder + '_backup'\n",
    "        if not os.path.exists(backup_folder):\n",
    "            print(f\"Creating backup of original files in: {backup_folder}\")\n",
    "            shutil.copytree(input_folder, backup_folder)\n",
    "            print(\"Backup complete!\")\n",
    "    \n",
    "    # Get all GeoTIFF files\n",
    "    tiff_files = glob(os.path.join(input_folder, \"Seoul_*_L8.tif\"))\n",
    "    print(f\"Found {len(tiff_files)} files to process\")\n",
    "    \n",
    "    # Track overall statistics\n",
    "    total_pixels_removed = 0\n",
    "    total_pixels_processed = 0\n",
    "    \n",
    "    for tiff_path in tqdm(tiff_files, desc=\"Cleaning LST files\"):\n",
    "        filename = os.path.basename(tiff_path)\n",
    "        # SAME filename as original\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        \n",
    "        with rasterio.open(tiff_path) as src:\n",
    "            # Read all bands\n",
    "            lst = src.read(1)  # Band 1: LST\n",
    "            ndvi = src.read(2)  # Band 2: NDVI  \n",
    "            sza = src.read(3)  # Band 3: Solar Zenith\n",
    "            dem = src.read(4)  # Band 4: DEM\n",
    "            \n",
    "            # Get metadata\n",
    "            meta = src.meta.copy()\n",
    "            \n",
    "            # Find valid LST pixels\n",
    "            valid_mask = ~np.isnan(lst)\n",
    "            valid_lst = lst[valid_mask]\n",
    "            \n",
    "            if len(valid_lst) == 0:\n",
    "                print(f\"  {filename}: No valid pixels, copying original\")\n",
    "                shutil.copy2(tiff_path, output_path)\n",
    "                continue\n",
    "            \n",
    "            # Calculate IQR for LST\n",
    "            q1 = np.percentile(valid_lst, 25)\n",
    "            q3 = np.percentile(valid_lst, 75)\n",
    "            iqr = q3 - q1\n",
    "            \n",
    "            # Define bounds\n",
    "            lower_bound = q1 - multiplier * iqr\n",
    "            upper_bound = q3 + multiplier * iqr\n",
    "            \n",
    "            # Find outliers\n",
    "            outlier_mask = (lst < lower_bound) | (lst > upper_bound)\n",
    "            n_outliers = np.sum(outlier_mask)\n",
    "            n_valid = np.sum(valid_mask)\n",
    "            \n",
    "            # Set outliers to NaN in all bands\n",
    "            lst_cleaned = lst.copy()\n",
    "            ndvi_cleaned = ndvi.copy()\n",
    "            sza_cleaned = sza.copy()\n",
    "            dem_cleaned = dem.copy()\n",
    "            \n",
    "            lst_cleaned[outlier_mask] = np.nan\n",
    "            ndvi_cleaned[outlier_mask] = np.nan\n",
    "            sza_cleaned[outlier_mask] = np.nan\n",
    "            dem_cleaned[outlier_mask] = np.nan\n",
    "            \n",
    "            # Additional check: Remove physically impossible values\n",
    "            # LST should be between -50°C and 70°C\n",
    "            physical_outliers = (lst_cleaned < -50) | (lst_cleaned > 70)\n",
    "            lst_cleaned[physical_outliers] = np.nan\n",
    "            ndvi_cleaned[physical_outliers] = np.nan\n",
    "            sza_cleaned[physical_outliers] = np.nan\n",
    "            dem_cleaned[physical_outliers] = np.nan\n",
    "            \n",
    "            # Update statistics\n",
    "            total_outliers = n_outliers + np.sum(physical_outliers)\n",
    "            total_pixels_removed += total_outliers\n",
    "            total_pixels_processed += n_valid\n",
    "            \n",
    "            # Print file statistics\n",
    "            outlier_pct = (total_outliers / n_valid * 100) if n_valid > 0 else 0\n",
    "            print(f\"  {filename}: {total_outliers:,} outliers ({outlier_pct:.1f}%) removed\")\n",
    "            print(f\"    LST range: [{valid_lst.min():.1f}, {valid_lst.max():.1f}]°C\")\n",
    "            print(f\"    IQR bounds: [{lower_bound:.1f}, {upper_bound:.1f}]°C\")\n",
    "            \n",
    "            # Write cleaned file with SAME name\n",
    "            with rasterio.open(output_path, 'w', **meta) as dst:\n",
    "                dst.write(lst_cleaned, 1)\n",
    "                dst.write(ndvi_cleaned, 2)\n",
    "                dst.write(sza_cleaned, 3)\n",
    "                dst.write(dem_cleaned, 4)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n=== CLEANING SUMMARY ===\")\n",
    "    print(f\"Total files processed: {len(tiff_files)}\")\n",
    "    print(f\"Total pixels processed: {total_pixels_processed:,}\")\n",
    "    print(f\"Total outliers removed: {total_pixels_removed:,}\")\n",
    "    print(f\"Overall outlier percentage: {(total_pixels_removed/total_pixels_processed*100):.2f}%\")\n",
    "    print(f\"Cleaned files saved to: {output_folder}\")\n",
    "    print(\"\\nIMPORTANT: Update your SATELLITE_FOLDER path to use the cleaned folder!\")\n",
    "\n",
    "def clean_in_place(folder_path, multiplier=1.5):\n",
    "    \"\"\"\n",
    "    Alternative: Clean files in-place (overwrites originals after backing up).\n",
    "    Use with caution!\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create backup first\n",
    "    backup_folder = folder_path + '_backup'\n",
    "    if not os.path.exists(backup_folder):\n",
    "        print(f\"Creating backup in: {backup_folder}\")\n",
    "        shutil.copytree(folder_path, backup_folder)\n",
    "        print(\"Backup complete!\")\n",
    "    else:\n",
    "        print(f\"Backup already exists at: {backup_folder}\")\n",
    "        response = input(\"Continue anyway? (y/n): \")\n",
    "        if response.lower() != 'y':\n",
    "            return\n",
    "    \n",
    "    # Clean files in the same folder\n",
    "    clean_lst_outliers_iqr(\n",
    "        input_folder=folder_path,\n",
    "        output_folder=folder_path,  # Same folder!\n",
    "        multiplier=multiplier,\n",
    "        create_backup=False  # Already created\n",
    "    )\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Define paths\n",
    "    INPUT_FOLDER = '/Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/Input Layer/Seoul Landsat'  # Your original files\n",
    "    OUTPUT_FOLDER = '/Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/Input Layer/Seoul Landsat Cleaned'  # Cleaned files\n",
    "    \n",
    "    print(\"Choose an option:\")\n",
    "    print(\"1. Clean to new folder (safer)\")\n",
    "    print(\"2. Clean in-place (overwrites originals after backup)\")\n",
    "    \n",
    "    choice = input(\"Enter choice (1 or 2): \")\n",
    "    \n",
    "    if choice == '1':\n",
    "        # Clean to new folder with same filenames\n",
    "        clean_lst_outliers_iqr(\n",
    "            input_folder=INPUT_FOLDER,\n",
    "            output_folder=OUTPUT_FOLDER,\n",
    "            multiplier=1.5\n",
    "        )\n",
    "        print(f\"\\n✓ Done! Update your scripts to use: '{OUTPUT_FOLDER}'\")\n",
    "        \n",
    "    elif choice == '2':\n",
    "        # Clean in-place\n",
    "        clean_in_place(INPUT_FOLDER, multiplier=1.5)\n",
    "        print(f\"\\n✓ Done! Files cleaned in-place. Backup saved at: '{INPUT_FOLDER}_backup'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4185d56b-1759-4803-8ade-49e0529bb9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting weather station data processing...\n",
      "Reading Landsat metadata...\n",
      "Found 95 Landsat images\n",
      "Reading station information...\n",
      "Found 34 stations\n",
      "Found 34 weather station files\n",
      "Processing station...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "LANDSAT_METADATA_PATH = '/Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/Input Layer/Seoul Landsat Metadata.csv'\n",
    "STATION_INFO_PATH = '/Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/AWS/Weather station data CVS.csv'\n",
    "WEATHER_DATA_FOLDER = '/Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/AWS/Weather station merging'\n",
    "OUTPUT_PATH = '/Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/landsat_matched_weather_data.csv'\n",
    "\n",
    "# Time tolerance for nearest match (in minutes)\n",
    "TIME_TOLERANCE = 30  # Will search ±30 minutes for nearest data\n",
    "\n",
    "def read_landsat_metadata(filepath):\n",
    "    \"\"\"Read and process Landsat metadata\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    # Ensure datetime column is parsed correctly\n",
    "    df['sensing_datetime'] = pd.to_datetime(df['Sensing Datetime'])\n",
    "    # Extract the minute from sensing time for matching\n",
    "    df['match_time'] = df['sensing_datetime'].dt.strftime('%H:%M')\n",
    "    return df\n",
    "\n",
    "def read_station_info(filepath):\n",
    "    \"\"\"Read station location information\"\"\"\n",
    "    stations = pd.read_csv(filepath)\n",
    "    # Create a dictionary for easy lookup\n",
    "    station_dict = {}\n",
    "    for _, row in stations.iterrows():\n",
    "        station_dict[row['Code']] = {\n",
    "            'name': row['Name'],\n",
    "            'latitude': row['Latitude'],\n",
    "            'longitude': row['Longitude'],\n",
    "            'altitude': row['Altitude']\n",
    "        }\n",
    "    return station_dict\n",
    "\n",
    "def extract_station_id_from_filename(filename):\n",
    "    \"\"\"Extract station ID from filename like '강남 400.csv'\"\"\"\n",
    "    # Remove .csv extension and split by space\n",
    "    base = os.path.basename(filename).replace('.csv', '')\n",
    "    # The ID is the number after the space\n",
    "    parts = base.split(' ')\n",
    "    if len(parts) >= 2:\n",
    "        try:\n",
    "            return int(parts[-1])  # Get the last part as ID\n",
    "        except:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def find_nearest_time_data(weather_df, target_datetime, time_tolerance_minutes=30):\n",
    "    \"\"\"Find the nearest available data within tolerance\"\"\"\n",
    "    # Create datetime index for efficient searching\n",
    "    weather_df['datetime'] = pd.to_datetime(weather_df['date and time'])\n",
    "    \n",
    "    # Calculate time differences\n",
    "    time_diff = abs((weather_df['datetime'] - target_datetime).dt.total_seconds() / 60)\n",
    "    \n",
    "    # Find minimum difference within tolerance\n",
    "    valid_mask = time_diff <= time_tolerance_minutes\n",
    "    if not valid_mask.any():\n",
    "        return None\n",
    "    \n",
    "    nearest_idx = time_diff[valid_mask].idxmin()\n",
    "    return weather_df.loc[nearest_idx]\n",
    "\n",
    "def process_weather_station_file(filepath, station_id, landsat_times, station_info):\n",
    "    \"\"\"Process individual weather station file\"\"\"\n",
    "    try:\n",
    "        # Read weather data\n",
    "        weather_df = pd.read_csv(filepath)\n",
    "        \n",
    "        # Rename columns if needed\n",
    "        weather_df.columns = ['station_id', 'date and time', 'temperature']\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for _, landsat_row in landsat_times.iterrows():\n",
    "            sensing_datetime = landsat_row['sensing_datetime']\n",
    "            \n",
    "            # Find nearest weather data\n",
    "            nearest_data = find_nearest_time_data(weather_df, sensing_datetime, TIME_TOLERANCE)\n",
    "            \n",
    "            if nearest_data is not None:\n",
    "                result = {\n",
    "                    'station_id': station_id,\n",
    "                    'sensing_datetime': sensing_datetime,\n",
    "                    'temperature': nearest_data['temperature'],\n",
    "                    'latitude': station_info[station_id]['latitude'],\n",
    "                    'longitude': station_info[station_id]['longitude'],\n",
    "                    'altitude': station_info[station_id]['altitude'],\n",
    "                    'actual_measurement_time': nearest_data['datetime'],\n",
    "                    'time_diff_minutes': abs((nearest_data['datetime'] - sensing_datetime).total_seconds() / 60)\n",
    "                }\n",
    "                results.append(result)\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filepath}: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main processing function\"\"\"\n",
    "    print(\"Starting weather station data processing...\")\n",
    "    \n",
    "    # Read Landsat metadata\n",
    "    print(\"Reading Landsat metadata...\")\n",
    "    landsat_df = read_landsat_metadata(LANDSAT_METADATA_PATH)\n",
    "    print(f\"Found {len(landsat_df)} Landsat images\")\n",
    "    \n",
    "    # Read station information\n",
    "    print(\"Reading station information...\")\n",
    "    station_info = read_station_info(STATION_INFO_PATH)\n",
    "    print(f\"Found {len(station_info)} stations\")\n",
    "    \n",
    "    # Get all weather station files\n",
    "    weather_files = glob.glob(os.path.join(WEATHER_DATA_FOLDER, '*.csv'))\n",
    "    print(f\"Found {len(weather_files)} weather station files\")\n",
    "    \n",
    "    # Process each weather station file\n",
    "    all_results = []\n",
    "    \n",
    "    for i, filepath in enumerate(weather_files):\n",
    "        station_id = extract_station_id_from_filename(filepath)\n",
    "        \n",
    "        if station_id and station_id in station_info:\n",
    "            print(f\"Processing station ...\")\n",
    "            station_results = process_weather_station_file(\n",
    "                filepath, station_id, landsat_df, station_info\n",
    "            )\n",
    "            all_results.append(station_results)\n",
    "        else:\n",
    "            print(f\"Skipping {filepath} - could not extract valid station ID\")\n",
    "    \n",
    "    # Combine all results\n",
    "    if all_results:\n",
    "        final_df = pd.concat(all_results, ignore_index=True)\n",
    "        \n",
    "        # Sort by datetime and station\n",
    "        final_df = final_df.sort_values(['sensing_datetime', 'station_id'])\n",
    "        \n",
    "        # Create summary statistics\n",
    "        print(\"\\n=== Summary Statistics ===\")\n",
    "        print(f\"Total matched records: {len(final_df)}\")\n",
    "        print(f\"Unique dates: {final_df['sensing_datetime'].nunique()}\")\n",
    "        print(f\"Unique stations: {final_df['station_id'].nunique()}\")\n",
    "        print(f\"Average time difference: {final_df['time_diff_minutes'].mean():.2f} minutes\")\n",
    "        print(f\"Max time difference: {final_df['time_diff_minutes'].max():.2f} minutes\")\n",
    "        \n",
    "        # Save the final dataset (without time_diff columns for cleaner output)\n",
    "        final_output = final_df[['station_id', 'sensing_datetime', 'temperature', \n",
    "                                'latitude', 'longitude', 'altitude']]\n",
    "        final_output.to_csv(OUTPUT_PATH, index=False)\n",
    "        print(f\"\\nFinal dataset saved to: {OUTPUT_PATH}\")\n",
    "        \n",
    "        # Also save a quality report\n",
    "        quality_report = final_df[['station_id', 'sensing_datetime', \n",
    "                                  'actual_measurement_time', 'time_diff_minutes']]\n",
    "        quality_report.to_csv('weather_matching_quality_report.csv', index=False)\n",
    "        print(f\"Quality report saved to: weather_matching_quality_report.csv\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No results to save!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043b85ed-5371-4277-bd5a-bc014f983fa0",
   "metadata": {},
   "source": [
    "Extract air temperature at 11:00 from S-DoT sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed6fbba4-a460-4b25-9b6b-cd40aa4b191b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting weather sensor data processing...\n",
      "Reading Landsat metadata...\n",
      "Found 95 Landsat images\n",
      "Reading sensor location information...\n",
      "Found 1095 sensors\n",
      "Processing 2013-05 (Landsat date: 2013-05-11)...\n",
      "  - WARNING: No sensor data file found for 2013-5\n",
      "Processing 2013-06 (Landsat date: 2013-06-28)...\n",
      "  - WARNING: No sensor data file found for 2013-6\n",
      "Processing 2013-09 (Landsat date: 2013-09-16)...\n",
      "  - WARNING: No sensor data file found for 2013-9\n",
      "Processing 2013-12 (Landsat date: 2013-12-21)...\n",
      "  - WARNING: No sensor data file found for 2013-12\n",
      "Processing 2014-01 (Landsat date: 2014-01-22)...\n",
      "  - WARNING: No sensor data file found for 2014-1\n",
      "Processing 2014-02 (Landsat date: 2014-02-23)...\n",
      "  - WARNING: No sensor data file found for 2014-2\n",
      "Processing 2014-03 (Landsat date: 2014-03-27)...\n",
      "  - WARNING: No sensor data file found for 2014-3\n",
      "Processing 2014-05 (Landsat date: 2014-05-30)...\n",
      "  - WARNING: No sensor data file found for 2014-5\n",
      "Processing 2014-07 (Landsat date: 2014-07-01)...\n",
      "  - WARNING: No sensor data file found for 2014-7\n",
      "Processing 2014-09 (Landsat date: 2014-09-19)...\n",
      "  - WARNING: No sensor data file found for 2014-9\n",
      "Processing 2014-10 (Landsat date: 2014-10-05)...\n",
      "  - WARNING: No sensor data file found for 2014-10\n",
      "Processing 2014-11 (Landsat date: 2014-11-06)...\n",
      "  - WARNING: No sensor data file found for 2014-11\n",
      "Processing 2014-12 (Landsat date: 2014-12-08)...\n",
      "  - WARNING: No sensor data file found for 2014-12\n",
      "Processing 2015-01 (Landsat date: 2015-01-09)...\n",
      "  - WARNING: No sensor data file found for 2015-1\n",
      "Processing 2015-02 (Landsat date: 2015-02-26)...\n",
      "  - WARNING: No sensor data file found for 2015-2\n",
      "Processing 2015-03 (Landsat date: 2015-03-14)...\n",
      "  - WARNING: No sensor data file found for 2015-3\n",
      "Processing 2015-05 (Landsat date: 2015-05-01)...\n",
      "  - WARNING: No sensor data file found for 2015-5\n",
      "Processing 2015-06 (Landsat date: 2015-06-18)...\n",
      "  - WARNING: No sensor data file found for 2015-6\n",
      "Processing 2015-07 (Landsat date: 2015-07-04)...\n",
      "  - WARNING: No sensor data file found for 2015-7\n",
      "Processing 2015-09 (Landsat date: 2015-09-22)...\n",
      "  - WARNING: No sensor data file found for 2015-9\n",
      "Processing 2015-10 (Landsat date: 2015-10-08)...\n",
      "  - WARNING: No sensor data file found for 2015-10\n",
      "Processing 2015-12 (Landsat date: 2015-12-27)...\n",
      "  - WARNING: No sensor data file found for 2015-12\n",
      "Processing 2016-01 (Landsat date: 2016-01-12)...\n",
      "  - WARNING: No sensor data file found for 2016-1\n",
      "Processing 2016-03 (Landsat date: 2016-03-16)...\n",
      "  - WARNING: No sensor data file found for 2016-3\n",
      "Processing 2016-04 (Landsat date: 2016-04-01)...\n",
      "  - WARNING: No sensor data file found for 2016-4\n",
      "Processing 2016-05 (Landsat date: 2016-05-19)...\n",
      "  - WARNING: No sensor data file found for 2016-5\n",
      "Processing 2016-07 (Landsat date: 2016-07-22)...\n",
      "  - WARNING: No sensor data file found for 2016-7\n",
      "Processing 2016-08 (Landsat date: 2016-08-07)...\n",
      "  - WARNING: No sensor data file found for 2016-8\n",
      "Processing 2016-09 (Landsat date: 2016-09-24)...\n",
      "  - WARNING: No sensor data file found for 2016-9\n",
      "Processing 2016-10 (Landsat date: 2016-10-10)...\n",
      "  - WARNING: No sensor data file found for 2016-10\n",
      "Processing 2016-11 (Landsat date: 2016-11-11)...\n",
      "  - WARNING: No sensor data file found for 2016-11\n",
      "Processing 2016-12 (Landsat date: 2016-12-29)...\n",
      "  - WARNING: No sensor data file found for 2016-12\n",
      "Processing 2017-01 (Landsat date: 2017-01-14)...\n",
      "  - WARNING: No sensor data file found for 2017-1\n",
      "Processing 2017-02 (Landsat date: 2017-02-15)...\n",
      "  - WARNING: No sensor data file found for 2017-2\n",
      "Processing 2017-03 (Landsat date: 2017-03-19)...\n",
      "  - WARNING: No sensor data file found for 2017-3\n",
      "Processing 2017-04 (Landsat date: 2017-04-04)...\n",
      "  - WARNING: No sensor data file found for 2017-4\n",
      "Processing 2017-05 (Landsat date: 2017-05-06)...\n",
      "  - WARNING: No sensor data file found for 2017-5\n",
      "Processing 2017-06 (Landsat date: 2017-06-23)...\n",
      "  - WARNING: No sensor data file found for 2017-6\n",
      "Processing 2017-07 (Landsat date: 2017-07-25)...\n",
      "  - WARNING: No sensor data file found for 2017-7\n",
      "Processing 2017-08 (Landsat date: 2017-08-10)...\n",
      "  - WARNING: No sensor data file found for 2017-8\n",
      "Processing 2017-10 (Landsat date: 2017-10-29)...\n",
      "  - WARNING: No sensor data file found for 2017-10\n",
      "Processing 2017-11 (Landsat date: 2017-11-30)...\n",
      "  - WARNING: No sensor data file found for 2017-11\n",
      "Processing 2017-12 (Landsat date: 2017-12-16)...\n",
      "  - WARNING: No sensor data file found for 2017-12\n",
      "Processing 2018-01 (Landsat date: 2018-01-01)...\n",
      "  - WARNING: No sensor data file found for 2018-1\n",
      "Processing 2018-02 (Landsat date: 2018-02-02)...\n",
      "  - WARNING: No sensor data file found for 2018-2\n",
      "Processing 2018-03 (Landsat date: 2018-03-06)...\n",
      "  - WARNING: No sensor data file found for 2018-3\n",
      "Processing 2018-05 (Landsat date: 2018-05-09)...\n",
      "  - WARNING: No sensor data file found for 2018-5\n",
      "Processing 2018-07 (Landsat date: 2018-07-28)...\n",
      "  - WARNING: No sensor data file found for 2018-7\n",
      "Processing 2018-09 (Landsat date: 2018-09-30)...\n",
      "  - WARNING: No sensor data file found for 2018-9\n",
      "Processing 2018-11 (Landsat date: 2018-11-01)...\n",
      "  - WARNING: No sensor data file found for 2018-11\n",
      "Processing 2018-12 (Landsat date: 2018-12-19)...\n",
      "  - WARNING: No sensor data file found for 2018-12\n",
      "Processing 2019-01 (Landsat date: 2019-01-20)...\n",
      "  - WARNING: No sensor data file found for 2019-1\n",
      "Processing 2019-02 (Landsat date: 2019-02-21)...\n",
      "  - WARNING: No sensor data file found for 2019-2\n",
      "Processing 2019-06 (Landsat date: 2019-06-13)...\n",
      "  - WARNING: No sensor data file found for 2019-6\n",
      "Processing 2019-10 (Landsat date: 2019-10-19)...\n",
      "  - WARNING: No sensor data file found for 2019-10\n",
      "Processing 2019-11 (Landsat date: 2019-11-20)...\n",
      "  - WARNING: No sensor data file found for 2019-11\n",
      "Processing 2019-12 (Landsat date: 2019-12-06)...\n",
      "  - WARNING: No sensor data file found for 2019-12\n",
      "Processing 2020-01 (Landsat date: 2020-01-23)...\n",
      "  - WARNING: No sensor data file found for 2020-1\n",
      "Processing 2020-02 (Landsat date: 2020-02-08)...\n",
      "  - WARNING: No sensor data file found for 2020-2\n",
      "Processing 2020-03 (Landsat date: 2020-03-11)...\n",
      "  - WARNING: No sensor data file found for 2020-3\n",
      "Processing 2020-04 (Landsat date: 2020-04-28)...\n",
      "  - Found 831 sensor readings\n",
      "Processing 2020-05 (Landsat date: 2020-05-30)...\n",
      "  - Found 824 sensor readings\n",
      "Processing 2020-07 (Landsat date: 2020-07-17)...\n",
      "  - WARNING: No sensor data file found for 2020-7\n",
      "Processing 2020-09 (Landsat date: 2020-09-19)...\n",
      "  - WARNING: No sensor data file found for 2020-9\n",
      "Processing 2020-10 (Landsat date: 2020-10-05)...\n",
      "  - WARNING: No sensor data file found for 2020-10\n",
      "Processing 2020-12 (Landsat date: 2020-12-24)...\n",
      "  - Found 526 sensor readings\n",
      "Processing 2021-01 (Landsat date: 2021-01-09)...\n",
      "  - WARNING: No sensor data file found for 2021-1\n",
      "Processing 2021-02 (Landsat date: 2021-02-26)...\n",
      "  - Found 1024 sensor readings\n",
      "Processing 2021-03 (Landsat date: 2021-03-30)...\n",
      "  - Found 1040 sensor readings\n",
      "Processing 2021-04 (Landsat date: 2021-04-15)...\n",
      "  - Found 1051 sensor readings\n",
      "Processing 2021-06 (Landsat date: 2021-06-02)...\n",
      "  - Found 1040 sensor readings\n",
      "Processing 2021-08 (Landsat date: 2021-08-05)...\n",
      "  - Found 73 sensor readings\n",
      "Processing 2021-10 (Landsat date: 2021-10-24)...\n",
      "  - Found 1044 sensor readings\n",
      "Processing 2021-11 (Landsat date: 2021-11-25)...\n",
      "  - WARNING: No sensor data file found for 2021-11\n",
      "Processing 2021-12 (Landsat date: 2021-12-27)...\n",
      "  - WARNING: No sensor data file found for 2021-12\n",
      "Processing 2022-01 (Landsat date: 2022-01-28)...\n",
      "  - Found 1044 sensor readings\n",
      "Processing 2022-04 (Landsat date: 2022-04-02)...\n",
      "  - Found 765 sensor readings\n",
      "Processing 2022-05 (Landsat date: 2022-05-04)...\n",
      "  - Found 1052 sensor readings\n",
      "Processing 2022-09 (Landsat date: 2022-09-09)...\n",
      "  - Found 1001 sensor readings\n",
      "Processing 2022-10 (Landsat date: 2022-10-11)...\n",
      "  - Found 1006 sensor readings\n",
      "Processing 2022-12 (Landsat date: 2022-12-30)...\n",
      "  - Found 966 sensor readings\n",
      "Processing 2023-04 (Landsat date: 2023-04-21)...\n",
      "  - WARNING: No sensor data file found for 2023-4\n",
      "Processing 2023-05 (Landsat date: 2023-05-23)...\n",
      "  - Found 816 sensor readings\n",
      "Processing 2023-06 (Landsat date: 2023-06-24)...\n",
      "  - WARNING: No sensor data file found for 2023-6\n",
      "Processing 2023-09 (Landsat date: 2023-09-28)...\n",
      "  - WARNING: No sensor data file found for 2023-9\n",
      "Processing 2023-10 (Landsat date: 2023-10-30)...\n",
      "  - Found 894 sensor readings\n",
      "Processing 2023-12 (Landsat date: 2023-12-01)...\n",
      "  - Found 882 sensor readings\n",
      "Processing 2024-01 (Landsat date: 2024-01-18)...\n",
      "  - Found 872 sensor readings\n",
      "Processing 2024-04 (Landsat date: 2024-04-07)...\n",
      "  - Found 931 sensor readings\n",
      "Processing 2024-05 (Landsat date: 2024-05-09)...\n",
      "  - Found 879 sensor readings\n",
      "Processing 2024-06 (Landsat date: 2024-06-26)...\n",
      "  - Found 900 sensor readings\n",
      "Processing 2024-08 (Landsat date: 2024-08-29)...\n",
      "  - Found 879 sensor readings\n",
      "Processing 2024-09 (Landsat date: 2024-09-30)...\n",
      "  - WARNING: No sensor data file found for 2024-9\n",
      "Processing 2024-10 (Landsat date: 2024-10-16)...\n",
      "  - Found 887 sensor readings\n",
      "Processing 2024-12 (Landsat date: 2024-12-19)...\n",
      "  - Found 973 sensor readings\n",
      "\n",
      "=== Summary Statistics ===\n",
      "Total matched records: 22200\n",
      "Unique Landsat dates: 25\n",
      "Unique sensors per date: 888\n",
      "Temperature range: -13.9 - 36.6°C\n",
      "Missing months: 70\n",
      "Missing: 2013-05, 2013-06, 2013-09, 2013-12, 2014-01, 2014-02, 2014-03, 2014-05, 2014-07, 2014-09...\n",
      "\n",
      "Final dataset saved to: /Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/Input Layer/landsat_matched_sensor_data.csv\n",
      "Coverage report saved to: sensor_coverage_report.csv\n",
      "\n",
      "=== IMPORTANT NOTE ===\n",
      "Weather sensors typically read ~1.8°C higher than standard weather stations\n",
      "This bias should be considered during model validation\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "LANDSAT_METADATA_PATH = '/Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/Input Layer/Seoul Landsat Metadata.csv'\n",
    "SENSOR_INFO_PATH = '/Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/Weather Sensor/Sensor Location.csv'  # Serial Number, Latitude, Longitude, Height\n",
    "SENSOR_DATA_FOLDER = '/Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/Weather Sensor/Data'  # Contains clean_YYYY_M.csv files\n",
    "OUTPUT_PATH = '/Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/Input Layer/landsat_matched_sensor_data.csv'\n",
    "\n",
    "def read_landsat_metadata(filepath):\n",
    "    \"\"\"Read and process Landsat metadata\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    # Parse datetime\n",
    "    df['sensing_datetime'] = pd.to_datetime(df['Sensing Datetime'])\n",
    "    # Extract date only (since sensors are at 11:00, we'll match by date)\n",
    "    df['sensing_date'] = df['sensing_datetime'].dt.date\n",
    "    return df\n",
    "\n",
    "def read_sensor_info(filepath):\n",
    "    \"\"\"Read sensor location information\"\"\"\n",
    "    sensors = pd.read_csv(filepath)\n",
    "    return sensors\n",
    "\n",
    "def get_sensor_data_for_month(year, month, sensor_data_folder):\n",
    "    \"\"\"Load sensor data for a specific year and month\"\"\"\n",
    "    # Try both single digit and zero-padded month formats\n",
    "    filenames = [\n",
    "        f'clean_{year}_{month}.csv',\n",
    "        f'clean_{year}_{month:02d}.csv'\n",
    "    ]\n",
    "    \n",
    "    for filename in filenames:\n",
    "        filepath = os.path.join(sensor_data_folder, filename)\n",
    "        if os.path.exists(filepath):\n",
    "            return pd.read_csv(filepath)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def merge_sensor_data_with_locations(sensor_data, sensor_info):\n",
    "    \"\"\"Merge temperature data with sensor locations\"\"\"\n",
    "    # Merge on Serial Number\n",
    "    merged = pd.merge(\n",
    "        sensor_data,\n",
    "        sensor_info,\n",
    "        on='Serial Number',\n",
    "        how='inner'\n",
    "    )\n",
    "    return merged\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main processing function\"\"\"\n",
    "    print(\"Starting weather sensor data processing...\")\n",
    "    \n",
    "    # Read Landsat metadata\n",
    "    print(\"Reading Landsat metadata...\")\n",
    "    landsat_df = read_landsat_metadata(LANDSAT_METADATA_PATH)\n",
    "    print(f\"Found {len(landsat_df)} Landsat images\")\n",
    "    \n",
    "    # Read sensor information\n",
    "    print(\"Reading sensor location information...\")\n",
    "    sensor_info = read_sensor_info(SENSOR_INFO_PATH)\n",
    "    print(f\"Found {len(sensor_info)} sensors\")\n",
    "    \n",
    "    # Process each Landsat date\n",
    "    all_results = []\n",
    "    missing_months = []\n",
    "    \n",
    "    for idx, landsat_row in landsat_df.iterrows():\n",
    "        year = landsat_row['Year']\n",
    "        month = landsat_row['Month']\n",
    "        sensing_datetime = landsat_row['sensing_datetime']\n",
    "        sensing_date = landsat_row['sensing_date']\n",
    "        \n",
    "        print(f\"Processing {year}-{month:02d} (Landsat date: {sensing_date})...\")\n",
    "        \n",
    "        # Get sensor data for this month\n",
    "        sensor_data = get_sensor_data_for_month(year, month, SENSOR_DATA_FOLDER)\n",
    "        \n",
    "        if sensor_data is not None:\n",
    "            # Merge with locations\n",
    "            merged_data = merge_sensor_data_with_locations(sensor_data, sensor_info)\n",
    "            \n",
    "            # Add Landsat sensing datetime\n",
    "            merged_data['sensing_datetime'] = sensing_datetime\n",
    "            merged_data['year'] = year\n",
    "            merged_data['month'] = month\n",
    "            \n",
    "            # Rename columns for consistency\n",
    "            merged_data = merged_data.rename(columns={\n",
    "                'Serial Number': 'sensor_id',\n",
    "                'Air Temperature': 'temperature',\n",
    "                'Latitude': 'latitude',\n",
    "                'Longitude': 'longitude',\n",
    "                'Height': 'altitude'\n",
    "            })\n",
    "            \n",
    "            all_results.append(merged_data)\n",
    "            print(f\"  - Found {len(merged_data)} sensor readings\")\n",
    "        else:\n",
    "            missing_months.append(f\"{year}-{month:02d}\")\n",
    "            print(f\"  - WARNING: No sensor data file found for {year}-{month}\")\n",
    "    \n",
    "    # Combine all results\n",
    "    if all_results:\n",
    "        final_df = pd.concat(all_results, ignore_index=True)\n",
    "        \n",
    "        # Select and order columns for final output\n",
    "        final_df = final_df[['sensor_id', 'sensing_datetime', 'temperature', \n",
    "                           'latitude', 'longitude', 'altitude']]\n",
    "        \n",
    "        # Sort by datetime and sensor\n",
    "        final_df = final_df.sort_values(['sensing_datetime', 'sensor_id'])\n",
    "        \n",
    "        # Create summary statistics\n",
    "        print(\"\\n=== Summary Statistics ===\")\n",
    "        print(f\"Total matched records: {len(final_df)}\")\n",
    "        print(f\"Unique Landsat dates: {final_df['sensing_datetime'].nunique()}\")\n",
    "        print(f\"Unique sensors per date: {len(final_df) / final_df['sensing_datetime'].nunique():.0f}\")\n",
    "        print(f\"Temperature range: {final_df['temperature'].min():.1f} - {final_df['temperature'].max():.1f}°C\")\n",
    "        print(f\"Missing months: {len(missing_months)}\")\n",
    "        if missing_months:\n",
    "            print(f\"Missing: {', '.join(missing_months[:10])}{'...' if len(missing_months) > 10 else ''}\")\n",
    "        \n",
    "        # Save the final dataset\n",
    "        final_df.to_csv(OUTPUT_PATH, index=False)\n",
    "        print(f\"\\nFinal dataset saved to: {OUTPUT_PATH}\")\n",
    "        \n",
    "        # Create a coverage report\n",
    "        coverage_report = final_df.groupby('sensing_datetime').agg({\n",
    "            'sensor_id': 'count',\n",
    "            'temperature': ['mean', 'std', 'min', 'max']\n",
    "        }).round(2)\n",
    "        coverage_report.columns = ['sensor_count', 'temp_mean', 'temp_std', 'temp_min', 'temp_max']\n",
    "        coverage_report.to_csv('sensor_coverage_report.csv')\n",
    "        print(f\"Coverage report saved to: sensor_coverage_report.csv\")\n",
    "        \n",
    "        # Note about bias\n",
    "        print(\"\\n=== IMPORTANT NOTE ===\")\n",
    "        print(\"Weather sensors typically read ~1.8°C higher than standard weather stations\")\n",
    "        print(\"This bias should be considered during model validation\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No results to save!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16e57d8e-0484-4203-8f5b-a7094d85a986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting satellite feature extraction...\n",
      "Satellite folder: /Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/Input Layer/Seoul Landsat Cleaned\n",
      "Output folder: /Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/MLP\n",
      "=== Preparing Training Data from Weather Stations ===\n",
      "Found 82 unique dates with weather station data\n",
      "Processed 2015-01: 34 stations\n",
      "Processed 2015-02: 34 stations\n",
      "Processed 2015-03: 34 stations\n",
      "Processed 2015-05: 34 stations\n",
      "Processed 2015-06: 34 stations\n",
      "Processed 2015-07: 34 stations\n",
      "Processed 2015-09: 34 stations\n",
      "Processed 2015-10: 34 stations\n",
      "Processed 2015-12: 33 stations\n",
      "Processed 2016-01: 33 stations\n",
      "Processed 2016-03: 34 stations\n",
      "Processed 2016-04: 34 stations\n",
      "Processed 2016-05: 34 stations\n",
      "Processed 2016-07: 34 stations\n",
      "Processed 2016-08: 34 stations\n",
      "Processed 2016-09: 34 stations\n",
      "Processed 2016-10: 34 stations\n",
      "Processed 2016-11: 34 stations\n",
      "Processed 2016-12: 34 stations\n",
      "Processed 2017-01: 34 stations\n",
      "Processed 2017-02: 34 stations\n",
      "Processed 2017-03: 34 stations\n",
      "Processed 2017-04: 34 stations\n",
      "Processed 2017-05: 34 stations\n",
      "Processed 2017-06: 34 stations\n",
      "Processed 2017-07: 34 stations\n",
      "Processed 2017-08: 34 stations\n",
      "Processed 2017-10: 33 stations\n",
      "Processed 2017-11: 34 stations\n",
      "Processed 2017-12: 34 stations\n",
      "Processed 2018-01: 34 stations\n",
      "Processed 2018-02: 34 stations\n",
      "Processed 2018-03: 34 stations\n",
      "Processed 2018-05: 34 stations\n",
      "Processed 2018-07: 34 stations\n",
      "Processed 2018-09: 34 stations\n",
      "Processed 2018-11: 34 stations\n",
      "Processed 2018-12: 33 stations\n",
      "Processed 2019-01: 33 stations\n",
      "Processed 2019-02: 33 stations\n",
      "Processed 2019-06: 33 stations\n",
      "Processed 2019-10: 33 stations\n",
      "Processed 2019-11: 33 stations\n",
      "Processed 2019-12: 33 stations\n",
      "Processed 2020-01: 33 stations\n",
      "Processed 2020-02: 33 stations\n",
      "Processed 2020-03: 33 stations\n",
      "Processed 2020-04: 33 stations\n",
      "Processed 2020-05: 32 stations\n",
      "Processed 2020-07: 33 stations\n",
      "Processed 2020-09: 33 stations\n",
      "Processed 2020-10: 32 stations\n",
      "Processed 2020-12: 33 stations\n",
      "Processed 2021-01: 33 stations\n",
      "Processed 2021-02: 33 stations\n",
      "Processed 2021-03: 33 stations\n",
      "Processed 2021-04: 33 stations\n",
      "Processed 2021-06: 32 stations\n",
      "Processed 2021-08: 33 stations\n",
      "Processed 2021-10: 33 stations\n",
      "Processed 2021-11: 33 stations\n",
      "Processed 2021-12: 33 stations\n",
      "Processed 2022-01: 33 stations\n",
      "Processed 2022-04: 33 stations\n",
      "Processed 2022-05: 33 stations\n",
      "Processed 2022-09: 33 stations\n",
      "Processed 2022-10: 32 stations\n",
      "Processed 2022-12: 33 stations\n",
      "Processed 2023-04: 33 stations\n",
      "Processed 2023-05: 33 stations\n",
      "Processed 2023-06: 32 stations\n",
      "Processed 2023-09: 32 stations\n",
      "Processed 2023-10: 31 stations\n",
      "Processed 2023-12: 33 stations\n",
      "Processed 2024-01: 33 stations\n",
      "Processed 2024-04: 33 stations\n",
      "Processed 2024-05: 33 stations\n",
      "Processed 2024-06: 33 stations\n",
      "Processed 2024-08: 33 stations\n",
      "Processed 2024-09: 32 stations\n",
      "Processed 2024-10: 33 stations\n",
      "Processed 2024-12: 33 stations\n",
      "Removed 292 rows with missing values\n",
      "\n",
      "=== Training Data Summary ===\n",
      "Total samples: 2439\n",
      "Date range: 2015-01-09 11:11:06 to 2024-12-19 11:11:04\n",
      "Unique stations: 34\n",
      "Unique dates: 82\n",
      "\n",
      "Feature statistics:\n",
      "               LST         NDVI  SolarZenith          DEM  temperature\n",
      "count  2439.000000  2439.000000  2439.000000  2439.000000  2439.000000\n",
      "mean     19.847076     0.137768    44.568756    65.435837    13.614596\n",
      "std      13.744615     0.078483    14.419090   104.597488    11.450316\n",
      "min     -14.195715    -0.015827    22.731506     5.000000   -13.400000\n",
      "25%       6.982337     0.084625    29.011135    23.000000     3.500000\n",
      "50%      21.208136     0.118362    45.518448    38.000000    14.800000\n",
      "75%      30.961456     0.179517    59.240177    61.000000    24.200000\n",
      "max      51.126064     0.470261    63.886208   617.000000    35.200000\n",
      "\n",
      "=== Preparing Validation Data from Weather Sensors ===\n",
      "Found 25 unique dates with sensor data\n",
      "Processed 2020-04: 831 sensors\n",
      "Processed 2020-05: 824 sensors\n",
      "Processed 2020-12: 526 sensors\n",
      "Processed 2021-02: 1024 sensors\n",
      "Processed 2021-03: 1040 sensors\n",
      "Processed 2021-04: 1051 sensors\n",
      "Processed 2021-06: 1040 sensors\n",
      "Processed 2021-08: 73 sensors\n",
      "Processed 2021-10: 1044 sensors\n",
      "Processed 2022-01: 1044 sensors\n",
      "Processed 2022-04: 765 sensors\n",
      "Processed 2022-05: 1052 sensors\n",
      "Processed 2022-09: 1001 sensors\n",
      "Processed 2022-10: 1006 sensors\n",
      "Processed 2022-12: 966 sensors\n",
      "Processed 2023-05: 816 sensors\n",
      "Processed 2023-10: 894 sensors\n",
      "Processed 2023-12: 882 sensors\n",
      "Processed 2024-01: 872 sensors\n",
      "Processed 2024-04: 931 sensors\n",
      "Processed 2024-05: 879 sensors\n",
      "Processed 2024-06: 900 sensors\n",
      "Processed 2024-08: 879 sensors\n",
      "Processed 2024-10: 887 sensors\n",
      "Processed 2024-12: 973 sensors\n",
      "\n",
      "=== Sensor Validation Data Summary ===\n",
      "Total samples: 20412\n",
      "Date range: 2020-04-28 11:10:32 to 2024-12-19 11:11:04\n",
      "Unique sensors: 1090\n",
      "Unique dates: 25\n",
      "\n",
      "Visualization saved to: /Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/MLP/data_distribution_analysis.png\n",
      "\n",
      "=== Feature Extraction Complete ===\n",
      "Training data: /Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/MLP/weather_station_training_data.csv\n",
      "Sensor validation data: /Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/MLP/weather_sensor_validation_data.csv\n",
      "\n",
      "Ready for MLP training!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio import sample\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration\n",
    "LANDSAT_METADATA_PATH = '/Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/Input Layer/Seoul Landsat Metadata.csv'\n",
    "WEATHER_STATION_PATH = '/Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/Input Layer/landsat_matched_weather_data.csv'  # From previous script\n",
    "SENSOR_DATA_PATH = '/Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/Input Layer/landsat_matched_sensor_data.csv'  # From previous script\n",
    "SATELLITE_FOLDER = '/Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/Input Layer/Seoul Landsat Cleaned'  # Your GeoTIFF files\n",
    "OUTPUT_FOLDER = '/Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/MLP'\n",
    "\n",
    "# Create output folder\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "def extract_satellite_values_at_points(tiff_path, coordinates):\n",
    "    \"\"\"Extract values from multi-band GeoTIFF at given coordinates\"\"\"\n",
    "    import rasterio\n",
    "    \n",
    "    with rasterio.open(tiff_path) as src:\n",
    "        # Use the generator function directly\n",
    "        sampled_values = [x for x in src.sample(coordinates)]\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        values_array = np.array(sampled_values)\n",
    "        \n",
    "        # Return as dataframe with band names\n",
    "        return pd.DataFrame(\n",
    "            values_array,\n",
    "            columns=['LST', 'NDVI', 'SolarZenith', 'DEM']\n",
    "        )\n",
    "\n",
    "def prepare_training_data():\n",
    "    \"\"\"Prepare training data from weather stations\"\"\"\n",
    "    print(\"=== Preparing Training Data from Weather Stations ===\")\n",
    "    \n",
    "    # Read data\n",
    "    weather_df = pd.read_csv(WEATHER_STATION_PATH)\n",
    "    weather_df['sensing_datetime'] = pd.to_datetime(weather_df['sensing_datetime'])\n",
    "    \n",
    "    # Get unique dates\n",
    "    unique_dates = weather_df['sensing_datetime'].unique()\n",
    "    print(f\"Found {len(unique_dates)} unique dates with weather station data\")\n",
    "    \n",
    "    all_training_data = []\n",
    "    \n",
    "    for date in unique_dates:\n",
    "        # Extract year and month for filename\n",
    "        year = pd.to_datetime(date).year\n",
    "        month = pd.to_datetime(date).month\n",
    "        \n",
    "        # Skip if before 2015 (no weather station data)\n",
    "        if year < 2015:\n",
    "            print(f\"Skipping {year}-{month:02d} (no weather station data)\")\n",
    "            continue\n",
    "            \n",
    "        # Construct filename\n",
    "        tiff_filename = f\"Seoul_{year}_{month:02d}_L8.tif\"\n",
    "        tiff_path = os.path.join(SATELLITE_FOLDER, tiff_filename)\n",
    "        \n",
    "        if not os.path.exists(tiff_path):\n",
    "            print(f\"Warning: Satellite image not found: {tiff_filename}\")\n",
    "            continue\n",
    "            \n",
    "        # Get weather data for this date\n",
    "        date_weather = weather_df[weather_df['sensing_datetime'] == date]\n",
    "        \n",
    "        # Extract coordinates\n",
    "        coordinates = [(row['longitude'], row['latitude']) for _, row in date_weather.iterrows()]\n",
    "        \n",
    "        # Extract satellite values\n",
    "        satellite_values = extract_satellite_values_at_points(tiff_path, coordinates)\n",
    "        \n",
    "        # Combine with weather data\n",
    "        combined = pd.concat([\n",
    "            date_weather.reset_index(drop=True),\n",
    "            satellite_values.reset_index(drop=True)\n",
    "        ], axis=1)\n",
    "        \n",
    "        all_training_data.append(combined)\n",
    "        print(f\"Processed {year}-{month:02d}: {len(combined)} stations\")\n",
    "    \n",
    "    # Combine all data\n",
    "    if all_training_data:\n",
    "        training_df = pd.concat(all_training_data, ignore_index=True)\n",
    "        \n",
    "        # Add derived features\n",
    "        training_df['year'] = training_df['sensing_datetime'].dt.year\n",
    "        training_df['month'] = training_df['sensing_datetime'].dt.month\n",
    "        training_df['day_of_year'] = training_df['sensing_datetime'].dt.dayofyear\n",
    "        \n",
    "        # Clean data - remove any rows with NaN values\n",
    "        initial_count = len(training_df)\n",
    "        training_df = training_df.dropna()\n",
    "        removed_count = initial_count - len(training_df)\n",
    "        \n",
    "        if removed_count > 0:\n",
    "            print(f\"Removed {removed_count} rows with missing values\")\n",
    "        \n",
    "        # Save training data\n",
    "        output_path = os.path.join(OUTPUT_FOLDER, 'weather_station_training_data.csv')\n",
    "        training_df.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"\\n=== Training Data Summary ===\")\n",
    "        print(f\"Total samples: {len(training_df)}\")\n",
    "        print(f\"Date range: {training_df['sensing_datetime'].min()} to {training_df['sensing_datetime'].max()}\")\n",
    "        print(f\"Unique stations: {training_df['station_id'].nunique()}\")\n",
    "        print(f\"Unique dates: {training_df['sensing_datetime'].nunique()}\")\n",
    "        print(f\"\\nFeature statistics:\")\n",
    "        print(training_df[['LST', 'NDVI', 'SolarZenith', 'DEM', 'temperature']].describe())\n",
    "        \n",
    "        return training_df\n",
    "    else:\n",
    "        print(\"No training data created!\")\n",
    "        return None\n",
    "\n",
    "def prepare_sensor_validation_data():\n",
    "    \"\"\"Prepare validation data from weather sensors\"\"\"\n",
    "    print(\"\\n=== Preparing Validation Data from Weather Sensors ===\")\n",
    "    \n",
    "    # Read sensor data\n",
    "    sensor_df = pd.read_csv(SENSOR_DATA_PATH)\n",
    "    sensor_df['sensing_datetime'] = pd.to_datetime(sensor_df['sensing_datetime'])\n",
    "    \n",
    "    # Get unique dates (2021-2024)\n",
    "    unique_dates = sensor_df['sensing_datetime'].unique()\n",
    "    print(f\"Found {len(unique_dates)} unique dates with sensor data\")\n",
    "    \n",
    "    all_sensor_data = []\n",
    "    \n",
    "    for date in unique_dates:\n",
    "        year = pd.to_datetime(date).year\n",
    "        month = pd.to_datetime(date).month\n",
    "        \n",
    "        # Construct filename\n",
    "        tiff_filename = f\"Seoul_{year}_{month:02d}_L8.tif\"\n",
    "        tiff_path = os.path.join(SATELLITE_FOLDER, tiff_filename)\n",
    "        \n",
    "        if not os.path.exists(tiff_path):\n",
    "            print(f\"Warning: Satellite image not found: {tiff_filename}\")\n",
    "            continue\n",
    "            \n",
    "        # Get sensor data for this date\n",
    "        date_sensors = sensor_df[sensor_df['sensing_datetime'] == date]\n",
    "        \n",
    "        # Extract coordinates\n",
    "        coordinates = [(row['longitude'], row['latitude']) for _, row in date_sensors.iterrows()]\n",
    "        \n",
    "        # Extract satellite values\n",
    "        satellite_values = extract_satellite_values_at_points(tiff_path, coordinates)\n",
    "        \n",
    "        # Combine with sensor data\n",
    "        combined = pd.concat([\n",
    "            date_sensors.reset_index(drop=True),\n",
    "            satellite_values.reset_index(drop=True)\n",
    "        ], axis=1)\n",
    "        \n",
    "        all_sensor_data.append(combined)\n",
    "        print(f\"Processed {year}-{month:02d}: {len(combined)} sensors\")\n",
    "    \n",
    "    # Combine all data\n",
    "    if all_sensor_data:\n",
    "        sensor_validation_df = pd.concat(all_sensor_data, ignore_index=True)\n",
    "        \n",
    "        # Add derived features\n",
    "        sensor_validation_df['year'] = sensor_validation_df['sensing_datetime'].dt.year\n",
    "        sensor_validation_df['month'] = sensor_validation_df['sensing_datetime'].dt.month\n",
    "        sensor_validation_df['day_of_year'] = sensor_validation_df['sensing_datetime'].dt.dayofyear\n",
    "        \n",
    "        # Clean data\n",
    "        sensor_validation_df = sensor_validation_df.dropna()\n",
    "        \n",
    "        # Save sensor validation data\n",
    "        output_path = os.path.join(OUTPUT_FOLDER, 'weather_sensor_validation_data.csv')\n",
    "        sensor_validation_df.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"\\n=== Sensor Validation Data Summary ===\")\n",
    "        print(f\"Total samples: {len(sensor_validation_df)}\")\n",
    "        print(f\"Date range: {sensor_validation_df['sensing_datetime'].min()} to {sensor_validation_df['sensing_datetime'].max()}\")\n",
    "        print(f\"Unique sensors: {sensor_validation_df['sensor_id'].nunique()}\")\n",
    "        print(f\"Unique dates: {sensor_validation_df['sensing_datetime'].nunique()}\")\n",
    "        \n",
    "        return sensor_validation_df\n",
    "    else:\n",
    "        print(\"No sensor validation data created!\")\n",
    "        return None\n",
    "\n",
    "def visualize_data_distribution(training_df, sensor_df):\n",
    "    \"\"\"Create visualizations of data distribution\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Temperature distributions\n",
    "    ax = axes[0, 0]\n",
    "    ax.hist(training_df['temperature'], bins=30, alpha=0.6, label='Weather Stations', color='blue')\n",
    "    if sensor_df is not None:\n",
    "        ax.hist(sensor_df['temperature'], bins=30, alpha=0.6, label='Weather Sensors', color='red')\n",
    "    ax.set_xlabel('Temperature (°C)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('Temperature Distribution')\n",
    "    ax.legend()\n",
    "    \n",
    "    # LST vs Air Temperature\n",
    "    ax = axes[0, 1]\n",
    "    ax.scatter(training_df['LST'], training_df['temperature'], alpha=0.5, s=10)\n",
    "    ax.set_xlabel('LST (°C)')\n",
    "    ax.set_ylabel('Air Temperature (°C)')\n",
    "    ax.set_title('LST vs Air Temperature')\n",
    "    ax.plot([0, 50], [0, 50], 'k--', alpha=0.3)\n",
    "    \n",
    "    # Temporal distribution\n",
    "    ax = axes[1, 0]\n",
    "    monthly_counts = training_df.groupby(['year', 'month']).size().reset_index(name='count')\n",
    "    ax.plot(range(len(monthly_counts)), monthly_counts['count'])\n",
    "    ax.set_xlabel('Month Index')\n",
    "    ax.set_ylabel('Number of Samples')\n",
    "    ax.set_title('Temporal Distribution of Training Data')\n",
    "    \n",
    "    # Feature correlations\n",
    "    ax = axes[1, 1]\n",
    "    features = ['LST', 'NDVI', 'SolarZenith', 'DEM', 'temperature']\n",
    "    corr_matrix = training_df[features].corr()\n",
    "    im = ax.imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    ax.set_xticks(range(len(features)))\n",
    "    ax.set_yticks(range(len(features)))\n",
    "    ax.set_xticklabels(features, rotation=45)\n",
    "    ax.set_yticklabels(features)\n",
    "    ax.set_title('Feature Correlations')\n",
    "    \n",
    "    # Add correlation values\n",
    "    for i in range(len(features)):\n",
    "        for j in range(len(features)):\n",
    "            ax.text(j, i, f'{corr_matrix.iloc[i, j]:.2f}', \n",
    "                   ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    plt.colorbar(im, ax=ax)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_FOLDER, 'data_distribution_analysis.png'), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"Starting satellite feature extraction...\")\n",
    "    print(f\"Satellite folder: {SATELLITE_FOLDER}\")\n",
    "    print(f\"Output folder: {OUTPUT_FOLDER}\")\n",
    "    \n",
    "    # Prepare training data from weather stations\n",
    "    training_df = prepare_training_data()\n",
    "    \n",
    "    # Prepare validation data from sensors\n",
    "    sensor_df = prepare_sensor_validation_data()\n",
    "    \n",
    "    # Create visualizations\n",
    "    if training_df is not None:\n",
    "        visualize_data_distribution(training_df, sensor_df)\n",
    "        print(f\"\\nVisualization saved to: {OUTPUT_FOLDER}/data_distribution_analysis.png\")\n",
    "    \n",
    "    print(\"\\n=== Feature Extraction Complete ===\")\n",
    "    print(f\"Training data: {OUTPUT_FOLDER}/weather_station_training_data.csv\")\n",
    "    print(f\"Sensor validation data: {OUTPUT_FOLDER}/weather_sensor_validation_data.csv\")\n",
    "    print(\"\\nReady for MLP training!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e4c8ad-1776-4755-916a-68013914d4f1",
   "metadata": {},
   "source": [
    "MLP Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05015355-f58a-4d58-98af-9c81327081d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MLP Training Pipeline...\n",
      "Output folder: /Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/MLP Result\n",
      "=== Starting MLP Training ===\n",
      "Loading training data...\n",
      "Loaded 2439 training samples\n",
      "Data split - Train: 1708, Val: 365, Test: 366\n",
      "Scaler saved to /Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/MLP Result/feature_scaler.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,521</span> (45.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,521\u001b[0m (45.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,521</span> (45.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,521\u001b[0m (45.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model...\n",
      "Epoch 1/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 260.5605 - mae: 13.1751 - val_loss: 37.3728 - val_mae: 5.0861 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 33.1618 - mae: 4.5271 - val_loss: 15.7437 - val_mae: 3.1573 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 20.4788 - mae: 3.4810 - val_loss: 10.3641 - val_mae: 2.5210 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 15.5703 - mae: 3.0022 - val_loss: 7.6424 - val_mae: 2.1613 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 12.8178 - mae: 2.7713 - val_loss: 6.8273 - val_mae: 2.0402 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 11.0777 - mae: 2.5728 - val_loss: 6.6448 - val_mae: 2.0080 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 10.4981 - mae: 2.4976 - val_loss: 5.7392 - val_mae: 1.8783 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.6568 - mae: 2.3690 - val_loss: 5.2874 - val_mae: 1.7990 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.3528 - mae: 2.3555 - val_loss: 5.3647 - val_mae: 1.8206 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.5972 - mae: 2.2489 - val_loss: 5.7328 - val_mae: 1.8626 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.2616 - mae: 2.2091 - val_loss: 5.0122 - val_mae: 1.7699 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.5798 - mae: 2.2588 - val_loss: 5.6067 - val_mae: 1.8360 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.8763 - mae: 2.1774 - val_loss: 5.4193 - val_mae: 1.8067 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.3048 - mae: 2.2124 - val_loss: 4.8209 - val_mae: 1.7139 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.8328 - mae: 2.1885 - val_loss: 5.8192 - val_mae: 1.8597 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.2643 - mae: 2.2124 - val_loss: 5.6428 - val_mae: 1.8124 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.8671 - mae: 2.1686 - val_loss: 5.8133 - val_mae: 1.8556 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.0246 - mae: 2.0498 - val_loss: 5.3155 - val_mae: 1.7755 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.5266 - mae: 2.0967 - val_loss: 6.2527 - val_mae: 1.9014 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.4484 - mae: 2.0776 - val_loss: 5.1107 - val_mae: 1.7266 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.0612 - mae: 2.0521 - val_loss: 4.7561 - val_mae: 1.6689 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.7796 - mae: 1.9744 - val_loss: 5.1855 - val_mae: 1.7417 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.4516 - mae: 2.1029 - val_loss: 6.1406 - val_mae: 1.8660 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.7051 - mae: 2.1171 - val_loss: 6.3262 - val_mae: 1.9034 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.3573 - mae: 1.9566 - val_loss: 5.4810 - val_mae: 1.7753 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.0175 - mae: 2.0252 - val_loss: 6.2427 - val_mae: 1.8822 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6723 - mae: 2.0049 - val_loss: 5.6916 - val_mae: 1.8079 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.0638 - mae: 2.0384 - val_loss: 6.5956 - val_mae: 1.9381 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.4415 - mae: 1.9315 - val_loss: 6.4452 - val_mae: 1.9238 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.3613 - mae: 1.9183 - val_loss: 6.3461 - val_mae: 1.9123 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.9964 - mae: 2.0087 - val_loss: 5.2913 - val_mae: 1.7542 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.2822 - mae: 1.9209 - val_loss: 5.6951 - val_mae: 1.8159 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.9165 - mae: 2.0021 - val_loss: 5.8170 - val_mae: 1.8363 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.2086 - mae: 1.9211 - val_loss: 6.4363 - val_mae: 1.9211 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.6361 - mae: 1.9823 - val_loss: 6.4742 - val_mae: 1.9350 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.3857 - mae: 1.9040 - val_loss: 5.4210 - val_mae: 1.7714 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.2176 - mae: 1.9067 - val_loss: 6.5057 - val_mae: 1.9409 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.8698 - mae: 1.9503 - val_loss: 6.7570 - val_mae: 1.9864 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.5087 - mae: 1.9376 - val_loss: 7.5326 - val_mae: 2.1111 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.9698 - mae: 1.8964 - val_loss: 5.9689 - val_mae: 1.8572 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.4287 - mae: 1.9278 - val_loss: 6.4379 - val_mae: 1.9282 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.9615 - mae: 1.8805 - val_loss: 6.1083 - val_mae: 1.8628 - learning_rate: 5.0000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.1829 - mae: 1.9095 - val_loss: 5.3676 - val_mae: 1.7477 - learning_rate: 5.0000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.8419 - mae: 1.8452 - val_loss: 6.6532 - val_mae: 1.9528 - learning_rate: 5.0000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.3390 - mae: 1.9188 - val_loss: 5.8939 - val_mae: 1.8253 - learning_rate: 5.0000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.8950 - mae: 1.8063 - val_loss: 6.2232 - val_mae: 1.8791 - learning_rate: 5.0000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.6762 - mae: 1.8214 - val_loss: 7.1038 - val_mae: 2.0295 - learning_rate: 5.0000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.2396 - mae: 1.9162 - val_loss: 5.6033 - val_mae: 1.7844 - learning_rate: 5.0000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.6670 - mae: 1.8109 - val_loss: 6.2381 - val_mae: 1.8875 - learning_rate: 5.0000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.4769 - mae: 1.7732 - val_loss: 6.5642 - val_mae: 1.9397 - learning_rate: 5.0000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.9916 - mae: 1.8632 - val_loss: 7.8626 - val_mae: 2.1418 - learning_rate: 5.0000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.9552 - mae: 1.8642 - val_loss: 6.8103 - val_mae: 1.9822 - learning_rate: 5.0000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.9396 - mae: 1.8314 - val_loss: 7.0902 - val_mae: 2.0215 - learning_rate: 5.0000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.1813 - mae: 1.8881 - val_loss: 6.8836 - val_mae: 1.9934 - learning_rate: 5.0000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.8650 - mae: 1.8629 - val_loss: 8.0023 - val_mae: 2.1664 - learning_rate: 5.0000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.0508 - mae: 1.8506 - val_loss: 7.9528 - val_mae: 2.1661 - learning_rate: 5.0000e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.6811 - mae: 1.8176 - val_loss: 6.8129 - val_mae: 1.9803 - learning_rate: 5.0000e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.8528 - mae: 1.8429 - val_loss: 7.0856 - val_mae: 2.0294 - learning_rate: 5.0000e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.5263 - mae: 1.8108 - val_loss: 6.7931 - val_mae: 1.9843 - learning_rate: 5.0000e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.6998 - mae: 1.8095 - val_loss: 5.2117 - val_mae: 1.7220 - learning_rate: 5.0000e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.4081 - mae: 1.7852 - val_loss: 6.7481 - val_mae: 1.9694 - learning_rate: 5.0000e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2903 - mae: 1.7458 - val_loss: 5.8418 - val_mae: 1.8162 - learning_rate: 2.5000e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.5941 - mae: 1.8047 - val_loss: 5.5408 - val_mae: 1.7653 - learning_rate: 2.5000e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.3860 - mae: 1.7320 - val_loss: 5.8523 - val_mae: 1.8176 - learning_rate: 2.5000e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.4248 - mae: 1.7829 - val_loss: 6.2843 - val_mae: 1.8865 - learning_rate: 2.5000e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.3292 - mae: 1.7542 - val_loss: 5.8135 - val_mae: 1.8114 - learning_rate: 2.5000e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.6377 - mae: 1.7986 - val_loss: 6.3306 - val_mae: 1.8947 - learning_rate: 2.5000e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.6089 - mae: 1.7815 - val_loss: 5.8360 - val_mae: 1.8116 - learning_rate: 2.5000e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.4046 - mae: 1.7760 - val_loss: 5.8680 - val_mae: 1.8173 - learning_rate: 2.5000e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.6718 - mae: 1.7958 - val_loss: 5.7211 - val_mae: 1.7905 - learning_rate: 2.5000e-04\n",
      "Epoch 71/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2752 - mae: 1.7492 - val_loss: 6.3413 - val_mae: 1.8928 - learning_rate: 2.5000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved to /Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/MLP Result/air_temperature_model.h5\n",
      "\n",
      "=== Test Set Evaluation ===\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "RMSE: 2.095°C\n",
      "MAE: 1.616°C\n",
      "R2: 0.9684\n",
      "CVRMSE: 15.50%\n",
      "Mean_Observed: 13.517°C\n",
      "\n",
      "=== Independent Validation with Weather Sensors ===\n",
      "Loaded 20412 sensor samples\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 289us/step\n",
      "\n",
      "=== Sensor Validation Results ===\n",
      "Mean bias (Sensor - Predicted): 0.98°C\n",
      "Bias standard deviation: 4.63°C\n",
      "Expected bias: ~1.8°C (sensors typically read higher)\n",
      "Actual vs Expected difference: 0.82°C\n",
      "\n",
      "Sensor validation metrics:\n",
      "RMSE: 4.730°C\n",
      "MAE: 3.870°C\n",
      "R2: 0.8070\n",
      "CVRMSE: 31.17%\n",
      "Mean_Observed: 15.178°C\n",
      "\n",
      "=== Training Complete ===\n",
      "Model saved to: /Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/MLP Result/air_temperature_model.h5\n",
      "Results saved to: /Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/MLP Result\n",
      "\n",
      "Ready for spatial prediction on all images!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# Configuration\n",
    "TRAINING_DATA_PATH = '/Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/MLP/weather_station_training_data.csv'\n",
    "SENSOR_DATA_PATH = '/Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/MLP/weather_sensor_validation_data.csv'\n",
    "OUTPUT_FOLDER = '/Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/MLP Result'\n",
    "MODEL_SAVE_PATH = os.path.join(OUTPUT_FOLDER, 'air_temperature_model.h5')\n",
    "SCALER_SAVE_PATH = os.path.join(OUTPUT_FOLDER, 'feature_scaler.pkl')\n",
    "\n",
    "# Create output folder\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate comprehensive metrics including CVRMSE\"\"\"\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # Calculate CVRMSE\n",
    "    mean_observed = np.mean(y_true)\n",
    "    cvrmse = (rmse / mean_observed) * 100  # as percentage\n",
    "    \n",
    "    return {\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'CVRMSE': cvrmse,\n",
    "        'Mean_Observed': mean_observed\n",
    "    }\n",
    "\n",
    "def create_mlp_model(input_dim):\n",
    "    \"\"\"Create MLP model architecture\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(1)  # Output: air temperature\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def prepare_features(df):\n",
    "    \"\"\"Prepare feature matrix and target variable\"\"\"\n",
    "    # Select features\n",
    "    feature_columns = ['LST', 'NDVI', 'SolarZenith', 'DEM']\n",
    "    X = df[feature_columns].values\n",
    "    y = df['temperature'].values\n",
    "    \n",
    "    return X, y, feature_columns\n",
    "\n",
    "def train_mlp_model():\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    print(\"=== Starting MLP Training ===\")\n",
    "    \n",
    "    # Load training data\n",
    "    print(\"Loading training data...\")\n",
    "    train_df = pd.read_csv(TRAINING_DATA_PATH)\n",
    "    print(f\"Loaded {len(train_df)} training samples\")\n",
    "    \n",
    "    # Prepare features\n",
    "    X, y, feature_columns = prepare_features(train_df)\n",
    "    \n",
    "    # Data Split
    "    # Split data: 70% train, 15% validation, 15% test\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.176, random_state=42)  # 0.176 of 0.85 = 0.15\n",
    "    \n",
    "    print(f\"Data split - Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Save scaler\n",
    "    joblib.dump(scaler, SCALER_SAVE_PATH)\n",
    "    print(f\"Scaler saved to {SCALER_SAVE_PATH}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = create_mlp_model(input_dim=len(feature_columns))\n",
    "    model.summary()\n",
    "    \n",
    "    # Define callbacks\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=50,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=20,\n",
    "        min_lr=0.00001\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nTraining model...\")\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        validation_data=(X_val_scaled, y_val),\n",
    "        epochs=200,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    model.save(MODEL_SAVE_PATH)\n",
    "    print(f\"\\nModel saved to {MODEL_SAVE_PATH}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\n=== Test Set Evaluation ===\")\n",
    "    y_pred_test = model.predict(X_test_scaled).flatten()\n",
    "    test_metrics = calculate_metrics(y_test, y_pred_test)\n",
    "    \n",
    "    for metric, value in test_metrics.items():\n",
    "        if metric == 'CVRMSE':\n",
    "            print(f\"{metric}: {value:.2f}%\")\n",
    "        elif metric == 'R2':\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"{metric}: {value:.3f}°C\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    create_training_plots(history, y_test, y_pred_test, test_metrics)\n",
    "    \n",
    "    return model, scaler, history, test_metrics\n",
    "\n",
    "def create_training_plots(history, y_test, y_pred_test, test_metrics):\n",
    "    \"\"\"Create comprehensive visualization plots\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Loss curves\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "    ax.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss (MSE)')\n",
    "    ax.set_title('Training and Validation Loss Curves')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. MAE curves\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(history.history['mae'], label='Training MAE', linewidth=2)\n",
    "    ax.plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('MAE (°C)')\n",
    "    ax.set_title('Training and Validation MAE Curves')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Scatter plot - Predicted vs Observed\n",
    "    ax = axes[1, 0]\n",
    "    ax.scatter(y_test, y_pred_test, alpha=0.5, s=20)\n",
    "    \n",
    "    # Add 1:1 line\n",
    "    min_val = min(y_test.min(), y_pred_test.min())\n",
    "    max_val = max(y_test.max(), y_pred_test.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
    "    \n",
    "    # Add metrics text\n",
    "    textstr = f'R² = {test_metrics[\"R2\"]:.3f}\\nRMSE = {test_metrics[\"RMSE\"]:.2f}°C\\nCVRMSE = {test_metrics[\"CVRMSE\"]:.1f}%'\n",
    "    ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=10,\n",
    "            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    ax.set_xlabel('Observed Temperature (°C)')\n",
    "    ax.set_ylabel('Predicted Temperature (°C)')\n",
    "    ax.set_title('Test Set: Predicted vs Observed')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Residual plot\n",
    "    ax = axes[1, 1]\n",
    "    residuals = y_test - y_pred_test\n",
    "    ax.scatter(y_pred_test, residuals, alpha=0.5, s=20)\n",
    "    ax.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "    \n",
    "    # Add ±2 RMSE lines\n",
    "    ax.axhline(y=2*test_metrics['RMSE'], color='orange', linestyle=':', alpha=0.5)\n",
    "    ax.axhline(y=-2*test_metrics['RMSE'], color='orange', linestyle=':', alpha=0.5)\n",
    "    \n",
    "    ax.set_xlabel('Predicted Temperature (°C)')\n",
    "    ax.set_ylabel('Residuals (°C)')\n",
    "    ax.set_title('Residual Plot')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_FOLDER, 'mlp_training_results.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def validate_with_sensors(model, scaler):\n",
    "    \"\"\"Validate model with independent sensor data\"\"\"\n",
    "    print(\"\\n=== Independent Validation with Weather Sensors ===\")\n",
    "    \n",
    "    # Load sensor data\n",
    "    sensor_df = pd.read_csv(SENSOR_DATA_PATH)\n",
    "    print(f\"Loaded {len(sensor_df)} sensor samples\")\n",
    "    \n",
    "    # Prepare features\n",
    "    X_sensor, y_sensor, _ = prepare_features(sensor_df)\n",
    "    \n",
    "    # Scale features\n",
    "    X_sensor_scaled = scaler.transform(X_sensor)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred_sensor = model.predict(X_sensor_scaled).flatten()\n",
    "    \n",
    "    # Calculate bias\n",
    "    bias = np.mean(y_sensor - y_pred_sensor)\n",
    "    bias_std = np.std(y_sensor - y_pred_sensor)\n",
    "    \n",
    "    print(f\"\\n=== Sensor Validation Results ===\")\n",
    "    print(f\"Mean bias (Sensor - Predicted): {bias:.2f}°C\")\n",
    "    print(f\"Bias standard deviation: {bias_std:.2f}°C\")\n",
    "    print(f\"Expected bias: ~1.8°C (sensors typically read higher)\")\n",
    "    print(f\"Actual vs Expected difference: {abs(bias - 1.8):.2f}°C\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    sensor_metrics = calculate_metrics(y_sensor, y_pred_sensor)\n",
    "    print(f\"\\nSensor validation metrics:\")\n",
    "    for metric, value in sensor_metrics.items():\n",
    "        if metric == 'CVRMSE':\n",
    "            print(f\"{metric}: {value:.2f}%\")\n",
    "        elif metric == 'R2':\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"{metric}: {value:.3f}°C\")\n",
    "    \n",
    "    # Create sensor validation plots\n",
    "    create_sensor_validation_plots(y_sensor, y_pred_sensor, bias, bias_std)\n",
    "    \n",
    "    return bias, sensor_metrics\n",
    "\n",
    "def create_sensor_validation_plots(y_sensor, y_pred_sensor, bias, bias_std):\n",
    "    \"\"\"Create plots for sensor validation\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # 1. Scatter plot\n",
    "    ax = axes[0]\n",
    "    ax.scatter(y_pred_sensor, y_sensor, alpha=0.3, s=10)\n",
    "    \n",
    "    # Add 1:1 line and bias line\n",
    "    min_val = min(y_sensor.min(), y_pred_sensor.min())\n",
    "    max_val = max(y_sensor.max(), y_pred_sensor.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='1:1 line')\n",
    "    ax.plot([min_val, max_val], [min_val + bias, max_val + bias], 'g--', linewidth=2, label=f'Bias: {bias:.2f}°C')\n",
    "    \n",
    "    ax.set_xlabel('Predicted Temperature (°C)')\n",
    "    ax.set_ylabel('Sensor Temperature (°C)')\n",
    "    ax.set_title('Sensor Validation: Predicted vs Observed')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Histogram of differences\n",
    "    ax = axes[1]\n",
    "    differences = y_sensor - y_pred_sensor\n",
    "    ax.hist(differences, bins=50, alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(x=bias, color='r', linestyle='--', linewidth=2, label=f'Mean: {bias:.2f}°C')\n",
    "    ax.axvline(x=1.8, color='g', linestyle='--', linewidth=2, label='Expected: 1.8°C')\n",
    "    ax.set_xlabel('Temperature Difference (Sensor - Predicted) (°C)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('Distribution of Temperature Differences')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Bias by temperature range\n",
    "    ax = axes[2]\n",
    "    temp_bins = np.arange(0, 40, 5)\n",
    "    bin_indices = np.digitize(y_pred_sensor, temp_bins)\n",
    "    \n",
    "    biases = []\n",
    "    bin_centers = []\n",
    "    for i in range(1, len(temp_bins)):\n",
    "        mask = bin_indices == i\n",
    "        if np.sum(mask) > 10:  # Only if enough samples\n",
    "            bin_bias = np.mean((y_sensor - y_pred_sensor)[mask])\n",
    "            biases.append(bin_bias)\n",
    "            bin_centers.append((temp_bins[i-1] + temp_bins[i]) / 2)\n",
    "    \n",
    "    ax.plot(bin_centers, biases, 'o-', linewidth=2, markersize=8)\n",
    "    ax.axhline(y=1.8, color='g', linestyle='--', linewidth=2, label='Expected bias: 1.8°C')\n",
    "    ax.set_xlabel('Temperature Range (°C)')\n",
    "    ax.set_ylabel('Mean Bias (°C)')\n",
    "    ax.set_title('Bias by Temperature Range')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_FOLDER, 'sensor_validation_analysis.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"Starting MLP Training Pipeline...\")\n",
    "    print(f\"Output folder: {OUTPUT_FOLDER}\")\n",
    "    \n",
    "    # Train model\n",
    "    model, scaler, history, test_metrics = train_mlp_model()\n",
    "    \n",
    "    # Validate with sensors\n",
    "    bias, sensor_metrics = validate_with_sensors(model, scaler)\n",
    "    \n",
    "    # Save summary report\n",
    "    summary = {\n",
    "        'Training Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'Test Set Metrics': test_metrics,\n",
    "        'Sensor Validation Metrics': sensor_metrics,\n",
    "        'Sensor Bias': bias,\n",
    "        'Model Architecture': model.to_json(),\n",
    "        'Total Parameters': model.count_params()\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    with open(os.path.join(OUTPUT_FOLDER, 'training_summary.json'), 'w') as f:\n",
    "        json.dump(summary, f, indent=4, default=str)\n",
    "    \n",
    "    print(\"\\n=== Training Complete ===\")\n",
    "    print(f\"Model saved to: {MODEL_SAVE_PATH}\")\n",
    "    print(f\"Results saved to: {OUTPUT_FOLDER}\")\n",
    "    print(\"\\nReady for spatial prediction on all images!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94033ad-7b2f-487b-860a-08adf65717c3",
   "metadata": {},
   "source": [
    "Training stopped at Epoch 71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0db973b-a408-4001-b05d-f86f52ddee4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Spatial Prediction for All Images ===\n",
      "Loading model and scaler...\n",
      "Found 95 images to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|████████████████████████| 95/95 [34:05<00:00, 21.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Complete ===\n",
      "Successfully processed: 95 images\n",
      "Output folder: /Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/Air Temperature\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "MODEL_PATH = '/Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/MLP Result/air_temperature_model.h5'\n",
    "SCALER_PATH = '/Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/MLP Result/feature_scaler.pkl'\n",
    "SATELLITE_FOLDER = '/Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/Input Layer/Seoul Landsat'\n",
    "LANDSAT_METADATA_PATH = '/Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/Input Layer/Seoul_Image_Metadata_LocalTime.csv'\n",
    "OUTPUT_FOLDER = '/Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/Air Temperature'\n",
    "SAMPLE_VISUALIZATION_FOLDER = os.path.join(OUTPUT_FOLDER, 'sample_visualizations')\n",
    "\n",
    "# Create output folders\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "os.makedirs(SAMPLE_VISUALIZATION_FOLDER, exist_ok=True)\n",
    "\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter, maximum_filter\n",
    "\n",
    "def apply_spatial_smoothing(air_temp, sigma=2.0):\n",
    "    \"\"\"Apply Gaussian smoothing to remove artifacts\"\"\"\n",
    "    valid_mask = ~np.isnan(air_temp)\n",
    "    temp_filled = air_temp.copy()\n",
    "    temp_filled[~valid_mask] = 0\n",
    "    smoothed = gaussian_filter(temp_filled, sigma=sigma)\n",
    "    smoothed[~valid_mask] = np.nan\n",
    "    return smoothed\n",
    "\n",
    "def apply_enhanced_vegetation_cooling(air_temp, ndvi, lst, month):\n",
    "    \"\"\"Enhanced correction that ensures forests are always cooler than urban areas\"\"\"\n",
    "    \n",
    "    # Identify urban vs vegetation areas using percentiles\n",
    "    ndvi_valid = ndvi[~np.isnan(ndvi)]\n",
    "    urban_threshold = np.percentile(ndvi_valid, 30)  # Bottom 30% = urban\n",
    "    forest_threshold = np.percentile(ndvi_valid, 70)  # Top 30% = forest\n",
    "    \n",
    "    # Calculate reference temperatures\n",
    "    urban_mask = ndvi < urban_threshold\n",
    "    urban_air_temp = air_temp[urban_mask]\n",
    "    urban_mean_temp = np.nanmean(urban_air_temp) if np.any(urban_mask) else np.nanmean(air_temp)\n",
    "    \n",
    "    # Define seasonal parameters\n",
    "    if month in [6, 7, 8]:  # Summer\n",
    "        base_cooling = -3.0\n",
    "        lst_coupling = 0.7\n",
    "    elif month in [12, 1, 2]:  # Winter\n",
    "        base_cooling = -1.0\n",
    "        lst_coupling = 0.9\n",
    "    else:  # Spring/Fall\n",
    "        base_cooling = -2.0\n",
    "        lst_coupling = 0.8\n",
    "    \n",
    "    # Create corrected temperature\n",
    "    corrected_temp = air_temp.copy()\n",
    "    \n",
    "    # Ensure forests are cooler than urban mean\n",
    "    forest_mask = ndvi > forest_threshold\n",
    "    if np.any(forest_mask):\n",
    "        max_forest_temp = urban_mean_temp + base_cooling\n",
    "        forest_air_from_lst = lst[forest_mask] * lst_coupling\n",
    "        corrected_temp[forest_mask] = np.minimum(\n",
    "            corrected_temp[forest_mask],\n",
    "            np.minimum(max_forest_temp, forest_air_from_lst)\n",
    "        )\n",
    "    \n",
    "    # Progressive cooling based on NDVI\n",
    "    ndvi_normalized = (ndvi - urban_threshold) / (forest_threshold - urban_threshold)\n",
    "    ndvi_normalized = np.clip(ndvi_normalized, 0, 1)\n",
    "    \n",
    "    # Get local urban temperature\n",
    "    local_urban_temp = maximum_filter(air_temp, size=5)\n",
    "    cooling_factor = base_cooling * ndvi_normalized\n",
    "    reference_temp = local_urban_temp + cooling_factor\n",
    "    corrected_temp = np.minimum(corrected_temp, reference_temp)\n",
    "    \n",
    "    # Use LST difference pattern\n",
    "    lst_local_max = maximum_filter(lst, size=5)\n",
    "    lst_cooling = lst - lst_local_max\n",
    "    air_temp_adjusted = corrected_temp + lst_cooling * 0.5\n",
    "    \n",
    "    vegetation_mask = ndvi > urban_threshold\n",
    "    corrected_temp[vegetation_mask] = np.minimum(\n",
    "        corrected_temp[vegetation_mask],\n",
    "        air_temp_adjusted[vegetation_mask]\n",
    "    )\n",
    "    \n",
    "    # Final constraint: air temp < LST in forests\n",
    "    corrected_temp[forest_mask] = np.minimum(\n",
    "        corrected_temp[forest_mask],\n",
    "        lst[forest_mask] - 1.0\n",
    "    )\n",
    "    \n",
    "    return corrected_temp\n",
    "\n",
    "def predict_air_temperature_for_image(model, scaler, tiff_path, year, month):\n",
    "    \"\"\"Apply MLP model with enhanced vegetation correction\"\"\"\n",
    "    \n",
    "    with rasterio.open(tiff_path) as src:\n",
    "        # Read all bands\n",
    "        lst = src.read(1)\n",
    "        ndvi = src.read(2)\n",
    "        solar_zenith = src.read(3)\n",
    "        dem = src.read(4)\n",
    "        \n",
    "        # Get metadata\n",
    "        height, width = lst.shape\n",
    "        transform = src.transform\n",
    "        crs = src.crs\n",
    "        \n",
    "        # Create valid pixel mask\n",
    "        valid_mask = (~np.isnan(lst) & \n",
    "                      ~np.isnan(ndvi) & \n",
    "                      ~np.isnan(solar_zenith) & \n",
    "                      ~np.isnan(dem))\n",
    "        \n",
    "        n_valid = np.sum(valid_mask)\n",
    "        if n_valid == 0:\n",
    "            return None\n",
    "        \n",
    "        # Stack features\n",
    "        features = np.stack([\n",
    "            lst[valid_mask],\n",
    "            ndvi[valid_mask],\n",
    "            solar_zenith[valid_mask],\n",
    "            dem[valid_mask]\n",
    "        ], axis=1)\n",
    "        \n",
    "        # Scale and predict\n",
    "        features_scaled = scaler.transform(features)\n",
    "        \n",
    "        # Predict in batches\n",
    "        batch_size = 10000\n",
    "        predictions = []\n",
    "        for i in range(0, n_valid, batch_size):\n",
    "            batch = features_scaled[i:i+batch_size]\n",
    "            batch_pred = model.predict(batch, verbose=0)\n",
    "            predictions.extend(batch_pred.flatten())\n",
    "        \n",
    "        # Create output array\n",
    "        air_temp = np.full((height, width), np.nan, dtype=np.float32)\n",
    "        air_temp[valid_mask] = predictions\n",
    "        \n",
    "    def remove_outliers_iqr(air_temp, lst, multiplier=1.5):\n",
    "        \"\"\"\n",
    "        Remove outliers based on the temperature difference using IQR method.\n",
    "        This should be applied after MLP prediction but before corrections.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Calculate temperature difference\n",
    "        temp_diff = lst - air_temp\n",
    "        \n",
    "        # Only consider valid pixels\n",
    "        valid_mask = ~np.isnan(temp_diff)\n",
    "        valid_diff = temp_diff[valid_mask]\n",
    "        \n",
    "        if len(valid_diff) == 0:\n",
    "            return air_temp\n",
    "        \n",
    "        # Calculate IQR\n",
    "        q1 = np.percentile(valid_diff, 25)\n",
    "        q3 = np.percentile(valid_diff, 75)\n",
    "        iqr = q3 - q1\n",
    "        \n",
    "        # Define outlier bounds\n",
    "        lower_bound = q1 - multiplier * iqr\n",
    "        upper_bound = q3 + multiplier * iqr\n",
    "        \n",
    "        # Identify outliers\n",
    "        outlier_mask = (temp_diff < lower_bound) | (temp_diff > upper_bound)\n",
    "        \n",
    "        # Set outliers to NaN\n",
    "        air_temp_cleaned = air_temp.copy()\n",
    "        air_temp_cleaned[outlier_mask] = np.nan\n",
    "        \n",
    "        # Also set corresponding LST to NaN for consistency\n",
    "        lst_cleaned = lst.copy()\n",
    "        lst_cleaned[outlier_mask] = np.nan\n",
    "        \n",
    "        # Print statistics\n",
    "        n_outliers = np.sum(outlier_mask)\n",
    "        n_total = np.sum(valid_mask)\n",
    "        outlier_pct = (n_outliers / n_total) * 100 if n_total > 0 else 0\n",
    "        \n",
    "        print(f\"  Outlier removal: {n_outliers:,} pixels ({outlier_pct:.1f}%) removed\")\n",
    "        print(f\"  IQR bounds: [{lower_bound:.1f}, {upper_bound:.1f}]°C difference\")\n",
    "        \n",
    "        return air_temp_cleaned, lst_cleaned, outlier_mask\n",
    "        \n",
    "        # Apply enhanced vegetation cooling\n",
    "        air_temp = apply_enhanced_vegetation_cooling(air_temp, ndvi, lst, month)\n",
    "        \n",
    "        # Apply final smoothing\n",
    "        air_temp = apply_spatial_smoothing(air_temp, sigma=0.7)\n",
    "        \n",
    "        # Check if correction worked (optional logging)\n",
    "        ndvi_valid = ndvi[~np.isnan(ndvi)]\n",
    "        urban_mask = ndvi < np.percentile(ndvi_valid, 30)\n",
    "        forest_mask = ndvi > np.percentile(ndvi_valid, 70)\n",
    "        \n",
    "        if np.any(urban_mask) and np.any(forest_mask):\n",
    "            urban_mean = np.nanmean(air_temp[urban_mask])\n",
    "            forest_mean = np.nanmean(air_temp[forest_mask])\n",
    "            if forest_mean < urban_mean:\n",
    "                print(f\"  ✓ {year}-{month:02d}: Forest ({forest_mean:.1f}°C) cooler than urban ({urban_mean:.1f}°C)\")\n",
    "            else:\n",
    "                print(f\"  ⚠ {year}-{month:02d}: Check needed - Forest still warmer\")\n",
    "        \n",
    "        return air_temp, transform, crs, valid_mask\n",
    "\n",
    "# Optional: Analyze NDVI distribution by season to verify thresholds\n",
    "def analyze_seasonal_ndvi(satellite_folder):\n",
    "    \"\"\"Analyze NDVI distribution across seasons\"\"\"\n",
    "    \n",
    "    seasonal_ndvi = {\n",
    "        'Spring': [],\n",
    "        'Summer': [],\n",
    "        'Fall': [],\n",
    "        'Winter': []\n",
    "    }\n",
    "    \n",
    "    for filename in os.listdir(satellite_folder):\n",
    "        if filename.endswith('_L8.tif'):\n",
    "            # Extract month\n",
    "            parts = filename.split('_')\n",
    "            month = int(parts[2])\n",
    "            \n",
    "            # Determine season\n",
    "            if month in [3, 4, 5]:\n",
    "                season = 'Spring'\n",
    "            elif month in [6, 7, 8]:\n",
    "                season = 'Summer'\n",
    "            elif month in [9, 10, 11]:\n",
    "                season = 'Fall'\n",
    "            else:\n",
    "                season = 'Winter'\n",
    "            \n",
    "            # Read NDVI\n",
    "            with rasterio.open(os.path.join(satellite_folder, filename)) as src:\n",
    "                ndvi = src.read(2)\n",
    "                valid_ndvi = ndvi[~np.isnan(ndvi)]\n",
    "                seasonal_ndvi[season].extend(valid_ndvi.flatten())\n",
    "    \n",
    "    # Print statistics\n",
    "    for season, values in seasonal_ndvi.items():\n",
    "        if values:\n",
    "            values = np.array(values)\n",
    "            print(f\"\\n{season} NDVI Statistics:\")\n",
    "            print(f\"  10th percentile: {np.percentile(values, 10):.3f}\")\n",
    "            print(f\"  50th percentile: {np.percentile(values, 50):.3f}\")\n",
    "            print(f\"  90th percentile: {np.percentile(values, 90):.3f}\")\n",
    "            print(f\"  95th percentile: {np.percentile(values, 95):.3f}\")\n",
    "\n",
    "\n",
    "def save_air_temperature_map(air_temp, transform, crs, output_path):\n",
    "    \"\"\"Save predicted air temperature as GeoTIFF\"\"\"\n",
    "    \n",
    "    # Define the metadata for the output raster\n",
    "    meta = {\n",
    "        'driver': 'GTiff',\n",
    "        'dtype': 'float32',\n",
    "        'nodata': np.nan,\n",
    "        'width': air_temp.shape[1],\n",
    "        'height': air_temp.shape[0],\n",
    "        'count': 1,\n",
    "        'crs': crs,\n",
    "        'transform': transform,\n",
    "        'compress': 'lzw'\n",
    "    }\n",
    "    \n",
    "    # Write the raster\n",
    "    with rasterio.open(output_path, 'w', **meta) as dst:\n",
    "        dst.write(air_temp, 1)\n",
    "\n",
    "def create_temperature_visualization(air_temp, lst, valid_mask, date_str, output_path):\n",
    "    \"\"\"Create a visualization comparing LST and predicted air temperature\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Common colormap settings\n",
    "    vmin = np.nanpercentile(air_temp[valid_mask], 1)\n",
    "    vmax = np.nanpercentile(air_temp[valid_mask], 99)\n",
    "    \n",
    "    # 1. LST\n",
    "    ax = axes[0]\n",
    "    im1 = ax.imshow(lst, cmap='RdYlBu_r', vmin=vmin, vmax=vmax)\n",
    "    ax.set_title(f'Land Surface Temperature\\n{date_str}', fontsize=12)\n",
    "    ax.axis('off')\n",
    "    plt.colorbar(im1, ax=ax, label='Temperature (°C)', fraction=0.046)\n",
    "    \n",
    "    # 2. Predicted Air Temperature\n",
    "    ax = axes[1]\n",
    "    im2 = ax.imshow(air_temp, cmap='RdYlBu_r', vmin=vmin, vmax=vmax)\n",
    "    ax.set_title(f'Predicted Air Temperature\\n{date_str}', fontsize=12)\n",
    "    ax.axis('off')\n",
    "    plt.colorbar(im2, ax=ax, label='Temperature (°C)', fraction=0.046)\n",
    "    \n",
    "    # 3. Difference (LST - Air Temp)\n",
    "    ax = axes[2]\n",
    "    diff = lst - air_temp\n",
    "    diff_vmax = np.nanpercentile(np.abs(diff[valid_mask]), 95)\n",
    "    im3 = ax.imshow(diff, cmap='RdBu_r', vmin=-diff_vmax, vmax=diff_vmax)\n",
    "    ax.set_title(f'LST - Air Temperature\\n{date_str}', fontsize=12)\n",
    "    ax.axis('off')\n",
    "    plt.colorbar(im3, ax=ax, label='Difference (°C)', fraction=0.046)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def process_all_images():\n",
    "    \"\"\"Process all satellite images to create air temperature maps\"\"\"\n",
    "    \n",
    "    print(\"=== Starting Spatial Prediction for All Images ===\")\n",
    "    \n",
    "    # Load model and scaler\n",
    "    print(\"Loading model and scaler...\")\n",
    "\n",
    "    model = tf.keras.models.load_model(MODEL_PATH, compile=False)\n",
    "    # Recompile with standard loss function\n",
    "    model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    scaler = joblib.load(SCALER_PATH)\n",
    "    \n",
    "    # Load metadata to get list of available images\n",
    "    metadata_df = pd.read_csv(LANDSAT_METADATA_PATH)\n",
    "    available_images = metadata_df[metadata_df['image_available'] == 'YES']\n",
    "    \n",
    "    print(f\"Found {len(available_images)} images to process\")\n",
    "    \n",
    "    # Process statistics\n",
    "    processing_stats = []\n",
    "    \n",
    "    # Process each image\n",
    "    for idx, row in tqdm(available_images.iterrows(), total=len(available_images), \n",
    "                         desc=\"Processing images\"):\n",
    "        \n",
    "        year = int(row['year'])  # Convert to int\n",
    "        month = int(row['month'])  # Convert to int\n",
    "        \n",
    "        # Construct filenames\n",
    "        input_filename = f\"Seoul_{year}_{month:02d}_L8.tif\"\n",
    "        output_filename = f\"Seoul_{year}_{month:02d}_AirTemp.tif\"\n",
    "        \n",
    "        \n",
    "        # Construct filenames\n",
    "        input_filename = f\"Seoul_{year}_{month:02d}_L8.tif\"\n",
    "        output_filename = f\"Seoul_{year}_{month:02d}_AirTemp.tif\"\n",
    "        \n",
    "        input_path = os.path.join(SATELLITE_FOLDER, input_filename)\n",
    "        output_path = os.path.join(OUTPUT_FOLDER, output_filename)\n",
    "        \n",
    "        if not os.path.exists(input_path):\n",
    "            print(f\"\\nWarning: Input file not found: {input_filename}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Predict air temperature\n",
    "            result = predict_air_temperature_for_image(model, scaler, input_path, year, month)\n",
    "            \n",
    "            if result is None:\n",
    "                continue\n",
    "                \n",
    "            air_temp, transform, crs, valid_mask = result\n",
    "            \n",
    "            # Save as GeoTIFF\n",
    "            save_air_temperature_map(air_temp, transform, crs, output_path)\n",
    "            \n",
    "            # Calculate statistics\n",
    "            stats = {\n",
    "                'year': year,\n",
    "                'month': month,\n",
    "                'date': row['date_acquired'],\n",
    "                'valid_pixels': np.sum(valid_mask),\n",
    "                'total_pixels': valid_mask.size,\n",
    "                'coverage_percent': (np.sum(valid_mask) / valid_mask.size) * 100,\n",
    "                'temp_min': np.nanmin(air_temp),\n",
    "                'temp_max': np.nanmax(air_temp),\n",
    "                'temp_mean': np.nanmean(air_temp),\n",
    "                'temp_std': np.nanstd(air_temp)\n",
    "            }\n",
    "            processing_stats.append(stats)\n",
    "            \n",
    "            # Create visualization for selected months\n",
    "            if idx % 12 == 0:  # Every 12th image\n",
    "                with rasterio.open(input_path) as src:\n",
    "                    lst = src.read(1)\n",
    "                \n",
    "                viz_path = os.path.join(SAMPLE_VISUALIZATION_FOLDER, \n",
    "                                       f\"comparison_{year}_{month:02d}.png\")\n",
    "                create_temperature_visualization(\n",
    "                    air_temp, lst, valid_mask, \n",
    "                    f\"{year}-{month:02d}\", viz_path\n",
    "                )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {input_filename}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Save processing statistics\n",
    "    stats_df = pd.DataFrame(processing_stats)\n",
    "    stats_df.to_csv(os.path.join(OUTPUT_FOLDER, 'processing_statistics.csv'), index=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n=== Processing Complete ===\")\n",
    "    print(f\"Successfully processed: {len(stats_df)} images\")\n",
    "    print(f\"Output folder: {OUTPUT_FOLDER}\")\n",
    "    \n",
    "    # Create seasonal summary\n",
    "    stats_df['season'] = stats_df['month'].map(\n",
    "        lambda m: 'Spring' if m in [3,4,5] else\n",
    "                  'Summer' if m in [6,7,8] else\n",
    "                  'Fall' if m in [9,10,11] else\n",
    "                  'Winter'\n",
    "    )\n",
    "    \n",
    "    print(\"\\nSeasonal Mean Temperatures:\")\n",
    "    seasonal_means = stats_df.groupby('season')['temp_mean'].agg(['mean', 'std', 'count'])\n",
    "    print(seasonal_means)\n",
    "    \n",
    "    return stats_df\n",
    "\n",
    "def create_time_series_visualization(stats_df):\n",
    "    \"\"\"Create time series plot of mean temperatures\"\"\"\n",
    "    \n",
    "    stats_df['date'] = pd.to_datetime(stats_df['date'])\n",
    "    stats_df = stats_df.sort_values('date')\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Plot mean temperature over time\n",
    "    plt.scatter(stats_df['date'], stats_df['temp_mean'], \n",
    "                c=stats_df['month'], cmap='hsv', s=50, alpha=0.7)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(stats_df.index, stats_df['temp_mean'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(stats_df['date'], p(stats_df.index), \"r--\", alpha=0.5, label=f'Trend: {z[0]:.3f}°C/image')\n",
    "    \n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Mean Air Temperature (°C)')\n",
    "    plt.title('Predicted Air Temperature Time Series for Study Area')\n",
    "    plt.colorbar(label='Month')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_FOLDER, 'temperature_time_series.png'), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Process all images\n",
    "    stats_df = process_all_images()\n",
    "    \n",
    "    # Create time series visualization\n",
    "    if len(stats_df) > 0:\n",
    "        create_time_series_visualization(stats_df)\n",
    "        \n",
    "        print(f\"\\nAll air temperature maps saved to: {OUTPUT_FOLDER}\")\n",
    "        print(f\"Sample visualizations saved to: {SAMPLE_VISUALIZATION_FOLDER}\")\n",
    "        print(f\"Statistics saved to: {os.path.join(OUTPUT_FOLDER, 'processing_statistics.csv')}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b287410e-79b0-4a72-8fbd-75832784c68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing LST to Air Temperature conversion...\n",
      "\n",
      "Analysis complete!\n",
      "Mean temperature decrease from LST to Air: 7.59°C\n",
      "Analysis saved to: /Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/Air Temperature/lst_to_air_temperature_analysis.png\n"
     ]
    }
   ],
   "source": [
    "def analyze_lst_to_air_temperature_conversion(satellite_folder, air_temp_folder, output_folder):\n",
    "    \"\"\"Analyze the temperature change from LST to final air temperature\"\"\"\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from glob import glob\n",
    "    \n",
    "    # Collect data from all processed images\n",
    "    all_lst = []\n",
    "    all_air_temp = []\n",
    "    all_differences = []\n",
    "    all_ndvi = []\n",
    "    all_months = []\n",
    "    \n",
    "    # Get list of processed files\n",
    "    air_temp_files = glob(os.path.join(air_temp_folder, \"*_AirTemp.tif\"))\n",
    "    \n",
    "    print(\"Analyzing LST to Air Temperature conversion...\")\n",
    "    \n",
    "    for air_temp_file in air_temp_files:\n",
    "        # Extract year and month from filename\n",
    "        basename = os.path.basename(air_temp_file)\n",
    "        parts = basename.split('_')\n",
    "        year = int(parts[1])\n",
    "        month = int(parts[2].replace('AirTemp.tif', ''))\n",
    "        \n",
    "        # Construct original satellite filename\n",
    "        sat_filename = f\"Seoul_{year}_{month:02d}_L8.tif\"\n",
    "        sat_path = os.path.join(satellite_folder, sat_filename)\n",
    "        \n",
    "        if not os.path.exists(sat_path):\n",
    "            continue\n",
    "            \n",
    "        # Read both files\n",
    "        with rasterio.open(sat_path) as src:\n",
    "            lst = src.read(1)\n",
    "            ndvi = src.read(2)\n",
    "            \n",
    "        with rasterio.open(air_temp_file) as src:\n",
    "            air_temp = src.read(1)\n",
    "        \n",
    "        # Get valid pixels (where both have data)\n",
    "        valid_mask = (~np.isnan(lst) & ~np.isnan(air_temp))\n",
    "        \n",
    "        # Sample pixels (to avoid memory issues with millions of pixels)\n",
    "        # Take every 10th pixel\n",
    "        sample_mask = valid_mask.copy()\n",
    "        sample_indices = np.where(valid_mask)\n",
    "        sample_step = 10\n",
    "        sample_mask[sample_indices[0][::sample_step], sample_indices[1][::sample_step]] = True\n",
    "        sample_mask[~valid_mask] = False\n",
    "        \n",
    "        # Collect data\n",
    "        lst_sample = lst[sample_mask]\n",
    "        air_temp_sample = air_temp[sample_mask]\n",
    "        ndvi_sample = ndvi[sample_mask]\n",
    "        difference = lst_sample - air_temp_sample\n",
    "        \n",
    "        all_lst.extend(lst_sample)\n",
    "        all_air_temp.extend(air_temp_sample)\n",
    "        all_differences.extend(difference)\n",
    "        all_ndvi.extend(ndvi_sample)\n",
    "        all_months.extend([month] * len(lst_sample))\n",
    "    \n",
    "    # Convert to arrays\n",
    "    all_lst = np.array(all_lst)\n",
    "    all_air_temp = np.array(all_air_temp)\n",
    "    all_differences = np.array(all_differences)\n",
    "    all_ndvi = np.array(all_ndvi)\n",
    "    all_months = np.array(all_months)\n",
    "    \n",
    "    # Create comprehensive figure\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Scatter plot: LST vs Air Temperature\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    # Sample for visualization (too many points otherwise)\n",
    "    sample_size = min(50000, len(all_lst))\n",
    "    idx = np.random.choice(len(all_lst), sample_size, replace=False)\n",
    "    \n",
    "    scatter = ax1.scatter(all_lst[idx], all_air_temp[idx], \n",
    "                         c=all_differences[idx], s=1, alpha=0.5, \n",
    "                         cmap='RdBu_r', vmin=-5, vmax=5)\n",
    "    ax1.plot([0, 50], [0, 50], 'k--', alpha=0.5, label='1:1 line')\n",
    "    ax1.set_xlabel('Land Surface Temperature (°C)')\n",
    "    ax1.set_ylabel('Air Temperature (°C)')\n",
    "    ax1.set_title('LST vs Air Temperature (All Pixels)')\n",
    "    plt.colorbar(scatter, ax=ax1, label='LST - Air Temp (°C)')\n",
    "    \n",
    "    # Add statistics\n",
    "    mean_diff = np.mean(all_differences)\n",
    "    std_diff = np.std(all_differences)\n",
    "    ax1.text(0.05, 0.95, f'Mean difference: {mean_diff:.2f}°C\\nStd: {std_diff:.2f}°C', \n",
    "             transform=ax1.transAxes, verticalalignment='top', \n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # 2. Histogram of differences\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    ax2.hist(all_differences, bins=100, alpha=0.7, edgecolor='black')\n",
    "    ax2.axvline(mean_diff, color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Mean: {mean_diff:.2f}°C')\n",
    "    ax2.set_xlabel('Temperature Difference (LST - Air Temp) (°C)')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Distribution of Temperature Differences')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Difference vs NDVI\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    # Bin NDVI and calculate mean difference\n",
    "    ndvi_bins = np.linspace(0, 1, 21)\n",
    "    ndvi_centers = (ndvi_bins[:-1] + ndvi_bins[1:]) / 2\n",
    "    mean_diffs = []\n",
    "    \n",
    "    for i in range(len(ndvi_bins)-1):\n",
    "        mask = (all_ndvi >= ndvi_bins[i]) & (all_ndvi < ndvi_bins[i+1])\n",
    "        if np.sum(mask) > 100:\n",
    "            mean_diffs.append(np.mean(all_differences[mask]))\n",
    "        else:\n",
    "            mean_diffs.append(np.nan)\n",
    "    \n",
    "    ax3.plot(ndvi_centers, mean_diffs, 'o-', linewidth=2, markersize=8)\n",
    "    ax3.set_xlabel('NDVI')\n",
    "    ax3.set_ylabel('Mean Temperature Difference (°C)')\n",
    "    ax3.set_title('Temperature Difference by Vegetation Level')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.axhline(0, color='k', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # 4. Seasonal variation\n",
    "    ax4 = plt.subplot(2, 3, 4)\n",
    "    seasons = {\n",
    "        'Winter': [12, 1, 2],\n",
    "        'Spring': [3, 4, 5],\n",
    "        'Summer': [6, 7, 8],\n",
    "        'Fall': [9, 10, 11]\n",
    "    }\n",
    "    \n",
    "    season_diffs = []\n",
    "    season_labels = []\n",
    "    \n",
    "    for season, months in seasons.items():\n",
    "        mask = np.isin(all_months, months)\n",
    "        if np.sum(mask) > 0:\n",
    "            season_diffs.append(all_differences[mask])\n",
    "            season_labels.append(f'{season}\\n({np.mean(all_differences[mask]):.2f}°C)')\n",
    "    \n",
    "    ax4.boxplot(season_diffs, labels=season_labels)\n",
    "    ax4.set_ylabel('Temperature Difference (°C)')\n",
    "    ax4.set_title('Seasonal Variation in LST-Air Temperature Difference')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.axhline(0, color='k', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # 5. Difference vs LST\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "    lst_bins = np.linspace(-10, 40, 26)\n",
    "    lst_centers = (lst_bins[:-1] + lst_bins[1:]) / 2\n",
    "    mean_diffs_by_lst = []\n",
    "    \n",
    "    for i in range(len(lst_bins)-1):\n",
    "        mask = (all_lst >= lst_bins[i]) & (all_lst < lst_bins[i+1])\n",
    "        if np.sum(mask) > 100:\n",
    "            mean_diffs_by_lst.append(np.mean(all_differences[mask]))\n",
    "        else:\n",
    "            mean_diffs_by_lst.append(np.nan)\n",
    "    \n",
    "    ax5.plot(lst_centers, mean_diffs_by_lst, 'o-', linewidth=2, markersize=8)\n",
    "    ax5.set_xlabel('Land Surface Temperature (°C)')\n",
    "    ax5.set_ylabel('Mean Temperature Difference (°C)')\n",
    "    ax5.set_title('Temperature Difference vs LST')\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    ax5.axhline(0, color='k', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # 6. Summary statistics table\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "    LST to Air Temperature Conversion Summary\n",
    "    =========================================\n",
    "    \n",
    "    Total pixels analyzed: {len(all_lst):,}\n",
    "    \n",
    "    Temperature Differences (LST - Air Temp):\n",
    "    • Mean: {np.mean(all_differences):.2f}°C\n",
    "    • Median: {np.median(all_differences):.2f}°C\n",
    "    • Std Dev: {np.std(all_differences):.2f}°C\n",
    "    • Min: {np.min(all_differences):.2f}°C\n",
    "    • Max: {np.max(all_differences):.2f}°C\n",
    "    \n",
    "    By Land Cover Type:\n",
    "    • Low NDVI (<0.2): {np.mean(all_differences[all_ndvi < 0.2]):.2f}°C\n",
    "    • Medium NDVI (0.2-0.3): {np.mean(all_differences[(all_ndvi >= 0.2) & (all_ndvi < 0.3)]):.2f}°C\n",
    "    • High NDVI (>0.3): {np.mean(all_differences[all_ndvi > 0.3]):.2f}°C\n",
    "    \n",
    "    Physical Interpretation:\n",
    "    • Positive values: LST > Air Temperature\n",
    "    • Negative values: LST < Air Temperature\n",
    "    • Larger differences in urban areas\n",
    "    • Smaller differences in vegetated areas\n",
    "    \"\"\"\n",
    "    \n",
    "    ax6.text(0.1, 0.9, summary_text, transform=ax6.transAxes, \n",
    "             fontsize=11, verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_folder, 'lst_to_air_temperature_analysis.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"\\nAnalysis complete!\")\n",
    "    print(f\"Mean temperature decrease from LST to Air: {np.mean(all_differences):.2f}°C\")\n",
    "    print(f\"Analysis saved to: {os.path.join(output_folder, 'lst_to_air_temperature_analysis.png')}\")\n",
    "\n",
    "# Call this function after processing all images\n",
    "analyze_lst_to_air_temperature_conversion(\n",
    "    satellite_folder=SATELLITE_FOLDER,\n",
    "    air_temp_folder=OUTPUT_FOLDER,\n",
    "    output_folder=OUTPUT_FOLDER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "075b76b1-6b53-4051-9a09-eb9c35d3f1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing LST to Air Temperature conversion...\n",
      "\n",
      "Removing outliers from temperature differences...\n",
      "Outlier removal statistics:\n",
      "  Total data points: 13,188,818\n",
      "  Outliers identified: 139,272 (1.06%)\n",
      "  IQR bounds: [-4.6, 19.8]°C\n",
      "\n",
      "Clean analysis complete!\n",
      "Results saved to: /Users/geunchansong/Documents/TU:d/Year 2/Graduation Thesis/LST to AT/Air Temperature/lst_to_air_temp_analysis_clean.png\n"
     ]
    }
   ],
   "source": [
    "def analyze_lst_to_air_temperature_conversion_with_outlier_removal(satellite_folder, air_temp_folder, output_folder):\n",
    "    \"\"\"Analyze temperature conversion with outlier removal for cleaner visualizations\"\"\"\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from glob import glob\n",
    "    from scipy import stats\n",
    "    \n",
    "    # Collect data from all processed images\n",
    "    all_lst = []\n",
    "    all_air_temp = []\n",
    "    all_differences = []\n",
    "    all_ndvi = []\n",
    "    all_months = []\n",
    "    \n",
    "    # Get list of processed files\n",
    "    air_temp_files = glob(os.path.join(air_temp_folder, \"*_AirTemp.tif\"))\n",
    "    \n",
    "    print(\"Analyzing LST to Air Temperature conversion...\")\n",
    "    \n",
    "    for air_temp_file in air_temp_files:\n",
    "        # Extract year and month from filename\n",
    "        basename = os.path.basename(air_temp_file)\n",
    "        parts = basename.split('_')\n",
    "        year = int(parts[1])\n",
    "        month = int(parts[2].replace('AirTemp.tif', ''))\n",
    "        \n",
    "        # Construct original satellite filename\n",
    "        sat_filename = f\"Seoul_{year}_{month:02d}_L8.tif\"\n",
    "        sat_path = os.path.join(satellite_folder, sat_filename)\n",
    "        \n",
    "        if not os.path.exists(sat_path):\n",
    "            continue\n",
    "            \n",
    "        # Read both files\n",
    "        with rasterio.open(sat_path) as src:\n",
    "            lst = src.read(1)\n",
    "            ndvi = src.read(2)\n",
    "            \n",
    "        with rasterio.open(air_temp_file) as src:\n",
    "            air_temp = src.read(1)\n",
    "        \n",
    "        # Get valid pixels\n",
    "        valid_mask = (~np.isnan(lst) & ~np.isnan(air_temp))\n",
    "        \n",
    "        # Sample pixels\n",
    "        sample_indices = np.where(valid_mask)\n",
    "        if len(sample_indices[0]) == 0:\n",
    "            continue\n",
    "            \n",
    "        sample_step = 10\n",
    "        sample_mask = np.zeros_like(valid_mask)\n",
    "        sample_mask[sample_indices[0][::sample_step], sample_indices[1][::sample_step]] = True\n",
    "        \n",
    "        # Collect data\n",
    "        lst_sample = lst[sample_mask]\n",
    "        air_temp_sample = air_temp[sample_mask]\n",
    "        ndvi_sample = ndvi[sample_mask]\n",
    "        difference = lst_sample - air_temp_sample\n",
    "        \n",
    "        all_lst.extend(lst_sample)\n",
    "        all_air_temp.extend(air_temp_sample)\n",
    "        all_differences.extend(difference)\n",
    "        all_ndvi.extend(ndvi_sample)\n",
    "        all_months.extend([month] * len(lst_sample))\n",
    "    \n",
    "    # Convert to arrays\n",
    "    all_lst = np.array(all_lst)\n",
    "    all_air_temp = np.array(all_air_temp)\n",
    "    all_differences = np.array(all_differences)\n",
    "    all_ndvi = np.array(all_ndvi)\n",
    "    all_months = np.array(all_months)\n",
    "    \n",
    "    if len(all_differences) == 0:\n",
    "        print(\"No data collected!\")\n",
    "        return\n",
    "    \n",
    "    # ============ OUTLIER REMOVAL ============\n",
    "    print(\"\\nRemoving outliers from temperature differences...\")\n",
    "    \n",
    "    # Method 1: IQR-based outlier removal\n",
    "    q1 = np.percentile(all_differences, 25)\n",
    "    q3 = np.percentile(all_differences, 75)\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    # Define bounds\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    \n",
    "    # Create mask for non-outliers\n",
    "    outlier_mask = (all_differences < lower_bound) | (all_differences > upper_bound)\n",
    "    clean_mask = ~outlier_mask\n",
    "    \n",
    "    # Print outlier statistics\n",
    "    n_outliers = np.sum(outlier_mask)\n",
    "    outlier_pct = (n_outliers / len(all_differences)) * 100\n",
    "    \n",
    "    print(f\"Outlier removal statistics:\")\n",
    "    print(f\"  Total data points: {len(all_differences):,}\")\n",
    "    print(f\"  Outliers identified: {n_outliers:,} ({outlier_pct:.2f}%)\")\n",
    "    print(f\"  IQR bounds: [{lower_bound:.1f}, {upper_bound:.1f}]°C\")\n",
    "    \n",
    "    # Apply mask to all arrays\n",
    "    all_lst_clean = all_lst[clean_mask]\n",
    "    all_air_temp_clean = all_air_temp[clean_mask]\n",
    "    all_differences_clean = all_differences[clean_mask]\n",
    "    all_ndvi_clean = all_ndvi[clean_mask]\n",
    "    all_months_clean = all_months[clean_mask]\n",
    "    \n",
    "    # ============ CREATE CLEAN VISUALIZATIONS ============\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Scatter plot\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    sample_size = min(50000, len(all_lst_clean))\n",
    "    if sample_size > 0:\n",
    "        idx = np.random.choice(len(all_lst_clean), sample_size, replace=False)\n",
    "        scatter = ax1.scatter(all_lst_clean[idx], all_air_temp_clean[idx], \n",
    "                             c=all_differences_clean[idx], s=1, alpha=0.5, \n",
    "                             cmap='RdBu_r', vmin=0, vmax=15)\n",
    "        ax1.plot([0, 50], [0, 50], 'k--', alpha=0.5, label='1:1 line')\n",
    "        plt.colorbar(scatter, ax=ax1, label='LST - Air Temp (°C)')\n",
    "    \n",
    "    ax1.set_xlabel('Land Surface Temperature (°C)')\n",
    "    ax1.set_ylabel('Air Temperature (°C)')\n",
    "    ax1.set_title('LST vs Air Temperature (Outliers Removed)')\n",
    "    \n",
    "    mean_diff_clean = np.mean(all_differences_clean)\n",
    "    std_diff_clean = np.std(all_differences_clean)\n",
    "    ax1.text(0.05, 0.95, f'Mean difference: {mean_diff_clean:.2f}°C\\nStd: {std_diff_clean:.2f}°C\\nOutliers removed: {outlier_pct:.1f}%', \n",
    "             transform=ax1.transAxes, verticalalignment='top', \n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # 2. Histogram\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    if len(all_differences_clean) > 0:\n",
    "        ax2.hist(all_differences_clean, bins=50, alpha=0.7, edgecolor='black', density=True)\n",
    "        ax2.axvline(mean_diff_clean, color='red', linestyle='--', linewidth=2, \n",
    "                    label=f'Mean: {mean_diff_clean:.2f}°C')\n",
    "        \n",
    "        # Add normal distribution overlay\n",
    "        x = np.linspace(all_differences_clean.min(), all_differences_clean.max(), 100)\n",
    "        ax2.plot(x, stats.norm.pdf(x, mean_diff_clean, std_diff_clean), 'g-', linewidth=2, \n",
    "                 label='Normal dist.')\n",
    "        ax2.legend()\n",
    "    \n",
    "    ax2.set_xlabel('Temperature Difference (LST - Air Temp) (°C)')\n",
    "    ax2.set_ylabel('Density')\n",
    "    ax2.set_title('Distribution of Temperature Differences (Cleaned)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Difference vs NDVI\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    ndvi_bins = np.linspace(0, 1, 21)\n",
    "    ndvi_centers = (ndvi_bins[:-1] + ndvi_bins[1:]) / 2\n",
    "    mean_diffs = []\n",
    "    \n",
    "    for i in range(len(ndvi_bins)-1):\n",
    "        mask = (all_ndvi_clean >= ndvi_bins[i]) & (all_ndvi_clean < ndvi_bins[i+1])\n",
    "        if np.sum(mask) > 100:\n",
    "            mean_diffs.append(np.mean(all_differences_clean[mask]))\n",
    "        else:\n",
    "            mean_diffs.append(np.nan)\n",
    "    \n",
    "    # Only plot non-NaN values\n",
    "    valid_points = ~np.isnan(mean_diffs)\n",
    "    if np.any(valid_points):\n",
    "        ax3.plot(ndvi_centers[valid_points], np.array(mean_diffs)[valid_points], \n",
    "                 'o-', linewidth=2, markersize=8)\n",
    "    \n",
    "    ax3.set_xlabel('NDVI')\n",
    "    ax3.set_ylabel('Mean Temperature Difference (°C)')\n",
    "    ax3.set_title('Temperature Difference by Vegetation Level (Cleaned)')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_ylim(0, 12)\n",
    "    \n",
    "    # 4. Seasonal variation\n",
    "    ax4 = plt.subplot(2, 3, 4)\n",
    "    seasons = {\n",
    "        'Winter': [12, 1, 2],\n",
    "        'Spring': [3, 4, 5],\n",
    "        'Summer': [6, 7, 8],\n",
    "        'Fall': [9, 10, 11]\n",
    "    }\n",
    "    \n",
    "    season_diffs = []\n",
    "    season_labels = []\n",
    "    \n",
    "    for season, months in seasons.items():\n",
    "        mask = np.isin(all_months_clean, months)\n",
    "        if np.sum(mask) > 0:\n",
    "            season_data = all_differences_clean[mask]\n",
    "            season_diffs.append(season_data)\n",
    "            season_labels.append(f'{season}\\n({np.mean(season_data):.2f}°C)')\n",
    "    \n",
    "    if len(season_diffs) > 0:\n",
    "        bp = ax4.boxplot(season_diffs, labels=season_labels, \n",
    "                         patch_artist=True, showfliers=False)\n",
    "        \n",
    "        colors = ['lightblue', 'lightgreen', 'lightyellow', 'peachpuff']\n",
    "        for patch, color in zip(bp['boxes'], colors[:len(bp['boxes'])]):\n",
    "            patch.set_facecolor(color)\n",
    "    \n",
    "    ax4.set_ylabel('Temperature Difference (°C)')\n",
    "    ax4.set_title('Seasonal Variation (Outliers Removed)')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.set_ylim(-5, 25)\n",
    "    \n",
    "    # 5. Before/After comparison\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "    ax5.hist(all_differences, bins=100, alpha=0.5, label='Original', \n",
    "             density=True, color='red', range=(-10, 20))\n",
    "    ax5.hist(all_differences_clean, bins=50, alpha=0.7, label='Cleaned', \n",
    "             density=True, color='blue', range=(-10, 20))\n",
    "    \n",
    "    ax5.set_xlabel('Temperature Difference (°C)')\n",
    "    ax5.set_ylabel('Density')\n",
    "    ax5.set_title('Before/After Outlier Removal')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    ax5.set_xlim(-10, 20)\n",
    "    \n",
    "    # 6. Summary statistics\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    # Calculate safe statistics\n",
    "    seasonal_stats = {}\n",
    "    for season, months in seasons.items():\n",
    "        mask = np.isin(all_months_clean, months)\n",
    "        if np.sum(mask) > 0:\n",
    "            seasonal_stats[season] = {\n",
    "                'mean': np.mean(all_differences_clean[mask]),\n",
    "                'std': np.std(all_differences_clean[mask]),\n",
    "                'median': np.median(all_differences_clean[mask])\n",
    "            }\n",
    "    \n",
    "    # Safe NDVI statistics\n",
    "    low_ndvi_mask = all_ndvi_clean < 0.2\n",
    "    med_ndvi_mask = (all_ndvi_clean >= 0.2) & (all_ndvi_clean < 0.3)\n",
    "    high_ndvi_mask = all_ndvi_clean > 0.3\n",
    "    \n",
    "    low_ndvi_mean = np.mean(all_differences_clean[low_ndvi_mask]) if np.any(low_ndvi_mask) else 0\n",
    "    med_ndvi_mean = np.mean(all_differences_clean[med_ndvi_mask]) if np.any(med_ndvi_mask) else 0\n",
    "    high_ndvi_mean = np.mean(all_differences_clean[high_ndvi_mask]) if np.any(high_ndvi_mask) else 0\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "    LST to Air Temperature Analysis (Outliers Removed)\n",
    "    ================================================\n",
    "    \n",
    "    Data Cleaning:\n",
    "    • Original points: {len(all_differences):,}\n",
    "    • Outliers removed: {n_outliers:,} ({outlier_pct:.1f}%)\n",
    "    • Clean data points: {len(all_differences_clean):,}\n",
    "    \n",
    "    Temperature Differences (LST - Air Temp):\n",
    "    • Mean: {np.mean(all_differences_clean):.2f}°C\n",
    "    • Median: {np.median(all_differences_clean):.2f}°C\n",
    "    • Std Dev: {np.std(all_differences_clean):.2f}°C\n",
    "    • Range: [{np.min(all_differences_clean):.2f}, {np.max(all_differences_clean):.2f}]°C\n",
    "    \n",
    "    Seasonal Means (Clean Data):\n",
    "    • Winter: {seasonal_stats.get('Winter', {}).get('mean', 0):.2f}°C\n",
    "    • Spring: {seasonal_stats.get('Spring', {}).get('mean', 0):.2f}°C\n",
    "    • Summer: {seasonal_stats.get('Summer', {}).get('mean', 0):.2f}°C\n",
    "    • Fall: {seasonal_stats.get('Fall', {}).get('mean', 0):.2f}°C\n",
    "    \n",
    "    By Land Cover (Clean Data):\n",
    "    • Low NDVI (<0.2): {low_ndvi_mean:.2f}°C\n",
    "    • Medium NDVI (0.2-0.3): {med_ndvi_mean:.2f}°C\n",
    "    • High NDVI (>0.3): {high_ndvi_mean:.2f}°C\n",
    "    \"\"\"\n",
    "    \n",
    "    ax6.text(0.1, 0.9, summary_text, transform=ax6.transAxes, \n",
    "             fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.suptitle('LST to Air Temperature Conversion Analysis - Outliers Removed', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_folder, 'lst_to_air_temp_analysis_clean.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"\\nClean analysis complete!\")\n",
    "    print(f\"Results saved to: {os.path.join(output_folder, 'lst_to_air_temp_analysis_clean.png')}\")\n",
    "\n",
    "# Run the analysis\n",
    "analyze_lst_to_air_temperature_conversion_with_outlier_removal(\n",
    "    satellite_folder=SATELLITE_FOLDER,\n",
    "    air_temp_folder=OUTPUT_FOLDER,\n",
    "    output_folder=OUTPUT_FOLDER\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
